{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5D Segmentation of lungs affected by Covid-19 pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "The following code is implemented to be runned all in once by setting tasks and their parameters.\n",
    "\n",
    "This notebook has been made to test different pre-processing techniques since it processes images in real-time and takes as input a 2D slice in `.npy` format. Inputs are statically made from orignal raw CT-scans by settings `preprocess_dataset=True`.\n",
    "\n",
    "Use the panel below to set the desired settings.\n",
    "\n",
    "In case of fine-tuning or evaluation, more settings must be edited in the relative section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dice Coefficient taken from https://kornia.readthedocs.io/en/v0.1.2/_modules/torchgeometry/losses/dice.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNZ6eNWFntkZ",
    "outputId": "389edc03-1772-45e1-eff0-0c23f68ceb07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------PIPELINE----------------\n",
      "\n",
      "Do Training: False\n",
      "Do Fine tuning: False\n",
      "Saving enabled: False\n",
      "Do Evaluation: False\n",
      "\n",
      "---------------PROCESSING---------------\n",
      "\n",
      "Normalization method: as_colab\n",
      "Equalization method: clahe+histeq\n",
      "\n",
      "---------------APPROACH---------------\n",
      "\n",
      "Approach: 2.5D\n",
      "Network: resnet101+Unet\n",
      "\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------DO---------------------------------------------------------------\n",
    "\n",
    "save_training = False # Either save the model during the training phase or not\n",
    "do_train = False # Perform the training \n",
    "do_fine_tuning = False # Perform the fine-tuning. More settings for fine-tuning must be setted in the relative section\n",
    "do_predict = False # Perform evaluation. More settings for evaluation must be setted in the relative section\n",
    "\n",
    "preprocess_dataset = False # Do only once, prepare ct-scans by rescaling them to the desired shape and store them as .npy\n",
    "\n",
    "#-------------------------------------------------NORMALIZATION---------------------------------------------------------\n",
    "\n",
    "eq_method = 'clahe+histeq' # Equalization method. Only if images are normalized in [0, 255]: None, histeq, clahe, clahe+histeq, histeq+clahe\n",
    "norm_method = 'as_colab' # Normalization method. Possible values: None, best or as_colab, mean_std, as_paper, as_paper_in_0_1, as_paper_in_0_255, in_range, custom, adjust_gamma, sharp\n",
    "lower, higher = 0, 255 # Lower and upper bounds in case of norm_method = in_range\n",
    "\n",
    "#---------------------------------------------------APPROACH------------------------------------------------------------\n",
    "\n",
    "approach = '2.5D' # Possible values: 2.5D, transformer\n",
    "\n",
    "#----------------------------------------------------NETWORK------------------------------------------------------------\n",
    "\n",
    "# For Encoder-Decoder\n",
    "encoder_name = 'resnet101' # Encoder to be used. Possible values: mobilenet_v2, resnet101, densenet169, vgg16 ...\n",
    "encoder_weights = 'imagenet' # Use either a pre-trained encoder or not. Possible values: imagenet, None\n",
    "decoder_name = 'Unet' # Decoder to be used. Possible values: Unet, MAnet, DeepLabV3, ...\n",
    "attention = None # Apply attention to the decoder. Possible values: scse, None\n",
    "batch_norm = True # Batch normalization technique. Possible values: True, inplace, False\n",
    "\n",
    "# For TransUnet  \n",
    "vit_name = 'R50+ViT-B_16' # Transformer configuration. Possible values: ViT-B_16, ViT-B_32, ViT-L_16, ViT-L_32, ViT-H_14, R50+ViT-B_16, testing\n",
    "vit_patches_size = 16 # Size of transformer's patches\n",
    "n_skip = 2 # Number of skip to be used\n",
    "\n",
    "#----------------------------------------------------DATASET------------------------------------------------------------\n",
    "\n",
    "n_classes = 3 # Total number of classes. 0:Background, 1:Lungs, 2:Infection\n",
    "shape = 512 # Shape of all the axes\n",
    "target_shape = (shape, shape, shape) # Volume shape to which rescale original ct-scans\n",
    "target_resolution = (334/shape, 334/shape, 1) # Resolution (mm^3) to which rescale original ct-scans\n",
    "merging_method = 'softmax' # Plurality voting merging method\n",
    "\n",
    "#-------------------------------------------------HYPERPRAMATERS--------------------------------------------------------\n",
    "\n",
    "loss_name = 'CE' # Loss function to be used. Possible values: WDL, DL, BCElogits, CE, WDL+CE, DL+CE, WCE\n",
    "sobel_loss = False # Add a Sobel error contribution to the loss function\n",
    "\n",
    "optimizer_name = 'ADAM' # Optimizer. Possible values: ADAM, ADAMW, SGD\n",
    "lr = 1e-3 # Learning rate\n",
    "weight_decay = 0 # Optimizer's weight decay\n",
    "\n",
    "use_scheduler = False # Use either a learning rate scheduler or not\n",
    "\n",
    "training_batch_size = 8 # Size of each train-set batch\n",
    "validation_batch_size = 8 # Size of each valid-set batch\n",
    "n_epochs = 30 # Total number of training epochs\n",
    "\n",
    "freeze_encoder = True # For Fine tuning; freeze encoder weights or not.\n",
    "\n",
    "dataset_name = 'challenge' # Dataset to be used for the training. Possible values: zenodo, challenge, zenodo+challenge\n",
    "\n",
    "#------------------------------------------------------PATHS------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "model_save_path = \"\" # Insert here the desired path to store trained models otherwise a default one will be used\n",
    "\n",
    "if model_save_path == \"\":\n",
    "    model_save_path = \"/home/\" + os.listdir(\"/home/\")[0] + \"/models/\"\n",
    "\n",
    "#---------------------------------------------------MODEL SAVE PREFIX---------------------------------------------------\n",
    "custom_prefix = '' # Insert here a desired prefix followed by '_'\n",
    "\n",
    "# Define model prefix based on some settings\n",
    "if norm_method == 'in_range':\n",
    "    model_prefix_name = eq_method + \"_\" + norm_method + \"[\" + str(lower) + ',' + str(higher) + ']_' \n",
    "else:\n",
    "    model_prefix_name = eq_method + \"_\" + norm_method + \"_\"\n",
    "if batch_norm == 'inplace':\n",
    "    model_prefix_name = 'bn_inplace_' + model_prefix_name\n",
    "if attention == 'scse':\n",
    "    model_prefix_name = 'scse_' + model_prefix_name\n",
    "if not freeze_encoder:\n",
    "    finetuning_prefix_name = 'all_trainable_' + 'finetuning_'\n",
    "else:\n",
    "    finetuning_prefix_name = 'encoder_freezed_' + 'finetuning_'\n",
    "\n",
    "model_prefix_name = custom_prefix +  model_prefix_name # Add the custom prefix\n",
    "\n",
    "#_______________________________________________________________________________________________________________________\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------SUMMARY------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n----------------PIPELINE----------------\\n\")\n",
    "print(\"Do Training: {}\".format(do_train))\n",
    "print(\"Do Fine tuning: {}\".format(do_fine_tuning))\n",
    "print(\"Saving enabled: {}\".format(save_training))\n",
    "print(\"Do Evaluation: {}\".format(do_predict))\n",
    "print(\"\\n---------------PROCESSING---------------\\n\")\n",
    "if norm_method == 'in_range':\n",
    "    print(\"Normalization method: in range [{},{}]\".format(lower,higher))\n",
    "else:\n",
    "    print(\"Normalization method: {}\".format(norm_method))\n",
    "print(\"Equalization method: {}\".format(eq_method))\n",
    "print(\"\\n---------------APPROACH---------------\\n\")\n",
    "print(\"Approach: {}\".format(approach))\n",
    "print(\"Network: {}\".format(vit_name)) if approach=='transformer' else print(\"Network: {}+{}\".format(encoder_name, decoder_name))\n",
    "if do_train or do_fine_tuning:\n",
    "    if do_train:\n",
    "        print(\"\\n---------------TRAINING---------------\\n\")\n",
    "    elif do_fine_tuning:\n",
    "        print(\"\\n--------------FINE TUNING-------------\\n\")\n",
    "    if do_fine_tuning:\n",
    "        print(\"Freeze encoder: {}\".format(freeze_encoder))\n",
    "    print(\"Training on: {}\".format(dataset_name))\n",
    "    print(\"Loss: {}+Sobel\".format(loss_name)) if sobel_loss else print(\"Loss: {}\".format(loss_name))\n",
    "    print(\"Learning Rate: {}\".format(lr))\n",
    "    print(\"Weight Decay: {}\".format(weight_decay))\n",
    "    print(\"Scheduler: {}\".format(use_scheduler))\n",
    "    print(\"Batch size Train/Valid: {}/{}\".format(training_batch_size,validation_batch_size))\n",
    "    print(\"Model savepath: {}\".format(model_save_path))\n",
    "    if do_train:\n",
    "        print(\"Model save prefix: {}\".format(model_prefix_name))\n",
    "    elif do_fine_tuning:\n",
    "        print(\"Model save prefix: {}\".format(finetuning_prefix_name))\n",
    "print(\"\\n------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlZ-raIRGZzj"
   },
   "source": [
    "# Globals and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e3CRiMmHs4T"
   },
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TzrNGdSP9fCz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import measure, color, exposure\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "from scipy import ndimage\n",
    "import scipy\n",
    "import itertools\n",
    "import math\n",
    "from os.path import join as pjoin\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "from torch.nn import Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "import ml_collections\n",
    "\n",
    "import libraries.segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQkFrtIAdP7y"
   },
   "source": [
    "## load Transformer pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--Y3poJ1dRyd",
    "outputId": "49198789-714f-4301-bdb5-fd6fcf973845"
   },
   "outputs": [],
   "source": [
    "# For TransUNet approach\n",
    "# Download pretrained weights for Transformer network\n",
    "if vit_name == 'ViT-H_14':\n",
    "    if not os.path.exists(model_save_path + 'vit_checkpoint/imagenet21k/' + vit_name + '.npz'):\n",
    "        !mkdir -p `echo $model_save_path`vit_checkpoint/imagenet21k\n",
    "        !wget https://storage.googleapis.com/vit_models/imagenet21k/{vit_name}.npz\n",
    "        !mv {vit_name}.npz `echo $model_save_path`vit_checkpoint/imagenet21k/{vit_name}.npz\n",
    "else:\n",
    "    if not os.path.exists(model_save_path + 'vit_checkpoint/imagenet21k+imagenet2012/' + vit_name +'.npz'):\n",
    "        !mkdir -p `echo $model_save_path`vit_checkpoint/imagenet21k+imagenet2012\n",
    "        !wget https://storage.googleapis.com/vit_models/imagenet21k+imagenet2012/{vit_name}.npz\n",
    "        !mv {vit_name}.npz `echo $model_save_path`vit_checkpoint/imagenet21k+imagenet2012/{vit_name}.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88WWUlgwH5FZ"
   },
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LVoKXA6pH8gp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32480MB, multi_processor_count=80\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select device\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Actual device: \", device)\n",
    "if 'cuda' in device:\n",
    "    print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
    "print(torch.cuda.memory_summary(device, abbreviated=True))\n",
    "\n",
    "# SEED\n",
    "seed = 169366\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6OG3Q_HGcp3"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural language sorting method\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets globals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zenodo -> files found: 10\n",
      "Challenge -> files found: 199\n"
     ]
    }
   ],
   "source": [
    "# Raw ZENODO .nii.gz\n",
    "zenodo_root = '/home/' + os.listdir(\"/home/\")[0] + '/datasets/zenodo/'\n",
    "zenodo_image_root = zenodo_root + 'image/'\n",
    "zenodo_mask_root = zenodo_root + 'mask/'\n",
    "zenodo = {'image': {'training': [], 'validation': []}, 'mask': {'training': [], 'validation': []}}\n",
    "zenodo['image']['training'] = [zenodo_image_root + 'training/' + f for f in os.listdir(zenodo_image_root + 'training/')]\n",
    "zenodo['image']['validation'] = [zenodo_image_root + 'validation/' + f for f in os.listdir(zenodo_image_root + 'validation/')]\n",
    "zenodo['mask']['training'] = [zenodo_mask_root + 'training/' + f for f in os.listdir(zenodo_mask_root + 'training/')]\n",
    "zenodo['mask']['validation'] = [zenodo_mask_root + 'validation/' + f for f in os.listdir(zenodo_mask_root + 'validation/')]      \n",
    "zenodo['image']['training'].sort(key=natural_keys)\n",
    "zenodo['mask']['training'].sort(key=natural_keys)\n",
    "zenodo['image']['validation'].sort(key=natural_keys)\n",
    "zenodo['mask']['validation'].sort(key=natural_keys)\n",
    "zenodo['mean'] = -558.356\n",
    "zenodo['std'] = 492.593\n",
    "print(\"Zenodo -> files found: {}\".format(len(zenodo['image']['training'])+len(zenodo['image']['validation'])))\n",
    "\n",
    "# Raw CHALLENGE .nii.gz\n",
    "challenge_root = '/home/' + os.listdir(\"/home/\")[0] + '/datasets/challenge/'\n",
    "challenge_image_root = challenge_root + 'image/'\n",
    "challenge_mask_root = challenge_root + 'mask/'\n",
    "challenge = {'image': {'training': [], 'validation': []}, 'mask': {'training': [], 'validation': []}}\n",
    "challenge['image']['training'] = [challenge_image_root + 'training/' + f for f in os.listdir(challenge_image_root + 'training/')]\n",
    "challenge['image']['validation'] = [challenge_image_root + 'validation/' + f for f in os.listdir(challenge_image_root + 'validation/')]\n",
    "challenge['mask']['training'] = [challenge_mask_root + 'training/' + f for f in os.listdir(challenge_mask_root + 'training/')]\n",
    "challenge['mask']['validation'] = [challenge_mask_root + 'validation/' + f for f in os.listdir(challenge_mask_root + 'validation/')]\n",
    "challenge['image']['training'].sort(key=natural_keys)\n",
    "challenge['mask']['training'].sort(key=natural_keys)\n",
    "challenge['image']['validation'].sort(key=natural_keys)\n",
    "challenge['mask']['validation'].sort(key=natural_keys)\n",
    "challenge['mean'] = -882.439\n",
    "challenge['std'] = 723.039 \n",
    "print(\"Challenge -> files found: {}\".format(len(challenge['image']['training'])+len(challenge['image']['validation'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zenodo -> files found: 10\n",
      "Challenge -> files found: 18\n"
     ]
    }
   ],
   "source": [
    "# ZENODO .npy\n",
    "zenodo_root_proc = '/home/' + os.listdir(\"/home/\")[0] + '/datasets/processed/npy/zenodo2D/'\n",
    "zenodo_image_root_proc = zenodo_root_proc + 'image/'\n",
    "zenodo_mask_root_proc = zenodo_root_proc + 'mask/'\n",
    "if os.path.isdir(zenodo_root_proc):\n",
    "    zenodo_proc = {'image': {'training': [], 'validation': []}, 'mask': {'training': [], 'validation': []}}\n",
    "    zenodo_proc['image']['training'] = [zenodo_image_root_proc + 'training/' + f for f in os.listdir(zenodo_image_root_proc + 'training/')]\n",
    "    zenodo_proc['image']['validation'] = [zenodo_image_root_proc + 'validation/' + f for f in os.listdir(zenodo_image_root_proc + 'validation/')]\n",
    "    zenodo_proc['mask']['training'] = [zenodo_mask_root_proc + 'training/' + f for f in os.listdir(zenodo_mask_root_proc + 'training/')]\n",
    "    zenodo_proc['mask']['validation'] = [zenodo_mask_root_proc + 'validation/' + f for f in os.listdir(zenodo_mask_root_proc + 'validation/')]      \n",
    "    zenodo_proc['image']['training'].sort(key=natural_keys)\n",
    "    zenodo_proc['mask']['training'].sort(key=natural_keys)\n",
    "    zenodo_proc['image']['validation'].sort(key=natural_keys)\n",
    "    zenodo_proc['mask']['validation'].sort(key=natural_keys)\n",
    "    zenodo_proc['mean'] = -558.356\n",
    "    zenodo_proc['std'] = 492.593\n",
    "    print(\"Zenodo -> files found: {}\".format(len(zenodo_proc['image']['training'])+len(zenodo_proc['image']['validation'])))\n",
    "\n",
    "# CHALLENGE .npy\n",
    "challenge_root_proc = '/home/' + os.listdir(\"/home/\")[0] + '/datasets/processed/npy/challenge2D/'\n",
    "challenge_image_root_proc = challenge_root_proc + 'image/'\n",
    "challenge_mask_root_proc = challenge_root_proc + 'mask/'\n",
    "if os.path.isdir(challenge_root_proc):\n",
    "    challenge_proc = {'image': {'training': [], 'validation': []}, 'mask': {'training': [], 'validation': []}}\n",
    "    challenge_proc['image']['training'] = [challenge_image_root_proc + 'training/' + f for f in os.listdir(challenge_image_root_proc + 'training/')]\n",
    "    challenge_proc['image']['validation'] = [challenge_image_root_proc + 'validation/' + f for f in os.listdir(challenge_image_root_proc + 'validation/')]\n",
    "    challenge_proc['mask']['training'] = [challenge_mask_root_proc + 'training/' + f for f in os.listdir(challenge_mask_root_proc + 'training/')]\n",
    "    challenge_proc['mask']['validation'] = [challenge_mask_root_proc + 'validation/' + f for f in os.listdir(challenge_mask_root_proc + 'validation/')]\n",
    "    challenge_proc['image']['training'].sort(key=natural_keys)\n",
    "    challenge_proc['mask']['training'].sort(key=natural_keys)\n",
    "    challenge_proc['image']['validation'].sort(key=natural_keys)\n",
    "    challenge_proc['mask']['validation'].sort(key=natural_keys)\n",
    "    challenge_proc['mean'] = -882.439\n",
    "    challenge_proc['std'] = 723.039 \n",
    "    print(\"Challenge -> files found: {}\".format(len(challenge_proc['image']['training'])+len(challenge_proc['image']['validation'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uO44zh6Hor5"
   },
   "source": [
    "## Dataset visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GJXq2uumGqV3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zenodo Dataset\n",
      "\n",
      "\n",
      "\n",
      "Challenge Dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_scan(filepath):\n",
    "    \"\"\"\n",
    "    Loads a volume from path. It can be either .npy or .nii.gz\n",
    "    \"\"\"\n",
    "    if filepath[-2:] == 'gz':\n",
    "        return nib.load(filepath)\n",
    "    elif filepath[-3:] == 'npy':\n",
    "        return np.load(filepath)\n",
    "\n",
    "def slice_image(image, idx_slice, axis='z'): \n",
    "    \"\"\"\n",
    "    Takes a volume slice.\n",
    "    ----------\n",
    "    Parameters:\n",
    "        image: nibabel object\n",
    "            The raw .nii.gz ct-scan\n",
    "        idx_slice: int\n",
    "            Index of the derided slice\n",
    "        axis: str, optional\n",
    "            Axis along which the image is being sliced\n",
    "    Returns:\n",
    "        The sliced 2D image as numpy array\n",
    "    \"\"\"\n",
    "    if axis == 'x':\n",
    "        cropped_img = image.slicer[idx_slice:idx_slice+1, :, :]\n",
    "    elif axis == 'y':\n",
    "        cropped_img = image.slicer[:, idx_slice:idx_slice+1, :]\n",
    "    elif axis == 'z':\n",
    "        cropped_img = image.slicer[:, :, idx_slice:idx_slice+1]\n",
    "    cropped_img = cropped_img.get_fdata().squeeze()\n",
    "    return cropped_img\n",
    "  \n",
    "def show_sample_shape(dataset, show_each_file=True):\n",
    "    \"\"\"\n",
    "    Shows shape, resolution and HU values of samples inside a dataset\n",
    "    ----------\n",
    "    Parameters:\n",
    "        dataset: dict\n",
    "            The dataset dict create above\n",
    "        show_each_file: bool, optional\n",
    "            Shows info about all the dataset samples if True, shows only a summary instead\n",
    "    \"\"\"\n",
    "    if show_each_file:\n",
    "        print('File:\\t\\t\\t\\t\\t Shape:\\t\\t\\t\\t Voxel dim:')\n",
    "    min_shape, min_res = (float('inf'), float('inf'), float('inf')), (float('inf'), float('inf'), float('inf'))\n",
    "    max_shape, max_res = (float('-inf'), float('-inf'), float('-inf')), (float('-inf'), float('-inf'), float('-inf'))\n",
    "    min_HU, max_HU = float('inf'), float('-inf')\n",
    "    mean, std = 0, 0\n",
    "    all_files = dataset['image']['training']\n",
    "    all_files.extend(dataset['image']['validation'])\n",
    "    for filename in all_files:\n",
    "        img = load_scan(filename)\n",
    "        header = img.header\n",
    "        img = img.get_fdata()\n",
    "        local_max, local_min = np.max(img), np.min(img)\n",
    "        mean += np.mean(img.flatten())\n",
    "        std += np.std(img.flatten())\n",
    "        if show_each_file:\n",
    "            print('{}\\t\\t {}\\t\\t {}'.format(filename.split(\"/\")[-1], img.shape, header.get_zooms()))\n",
    "        if sum(img.shape) < sum(min_shape):\n",
    "            min_shape = img.shape\n",
    "        if sum(header.get_zooms()) < sum(min_res):\n",
    "            min_res = header.get_zooms()\n",
    "        if sum(img.shape) > sum(max_shape):\n",
    "            max_shape = img.shape\n",
    "        if sum(header.get_zooms()) > sum(max_res):\n",
    "            max_res = header.get_zooms()\n",
    "        if local_max > max_HU:\n",
    "            max_HU = local_max\n",
    "        if local_min < min_HU:\n",
    "            min_HU = local_min\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------------------------------\")\n",
    "    print(\"Min shape: {}\\tMin Resolution: {}\\nMax shape: {}\\tMax Resolution: {}\".format(min_shape,min_res,max_shape,max_res))\n",
    "    print(\"Min HU val: {} \\t\\tMax HU val: {}\".format(min_HU, max_HU))\n",
    "    print(\"HU Mean: {:.3f}\\t\\tHU STD: {:.3f}\".format(mean/len(all_files), std/len(all_files)))\n",
    "    print(\"______________________________________________________________________________________________\")\n",
    "\n",
    "print(\"Zenodo Dataset\\n\")\n",
    "#show_sample_shape(zenodo)\n",
    "print(\"\\n\\nChallenge Dataset\\n\")\n",
    "#show_sample_shape(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xMLtepsK4Vp"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E-Bn9EfzkRfN"
   },
   "outputs": [],
   "source": [
    "def rescale_to_standard(array, resolution, target_resolution=(334/512, 334/512, 1), target_shape=(512, 512, 512)):\n",
    "    # pad and rescale the array to the same resolution and shape for further processing.\n",
    "    # input: array must has shape (x, y, z) and resolution is a list or tuple with three elements\n",
    "\n",
    "    original_shape = np.shape(array)\n",
    "    target_volume = (target_resolution[0]*target_shape[0], target_resolution[1]*target_shape[1], target_resolution[2]*target_shape[2])\n",
    "    shape_of_target_volume = (int(target_volume[0]/resolution[0]), int(target_volume[1]/resolution[1]), int(target_volume[2]/resolution[2]))\n",
    "\n",
    "    if original_shape[2] * resolution[2] > target_volume[2]:\n",
    "        warnings.warn('z-axis is longer than expectation. Make sure lung is near the center of z-axis.', SyntaxWarning)\n",
    "        array = array[:, :, 100::]\n",
    "        original_shape = np.shape(array)\n",
    "\n",
    "    x = max(shape_of_target_volume[0], original_shape[0]) + 2\n",
    "    y = max(shape_of_target_volume[1], original_shape[1]) + 2\n",
    "    z = max(shape_of_target_volume[2], original_shape[2]) + 2\n",
    "\n",
    "    x_start = int(x/2)-int(original_shape[0]/2)\n",
    "    x_end = x_start + original_shape[0]\n",
    "    y_start = int(y/2)-int(original_shape[1]/2)\n",
    "    y_end = y_start + original_shape[1]\n",
    "    z_start = int(z / 2) - int(original_shape[2] / 2)\n",
    "    z_end = z_start + original_shape[2]\n",
    "\n",
    "    array_intermediate = np.zeros((x, y, z), 'float32')\n",
    "    array_intermediate[x_start:x_end, y_start:y_end, z_start:z_end] = array\n",
    "\n",
    "    x_start = int(x / 2) - int(shape_of_target_volume[0] / 2)\n",
    "    x_end = x_start + shape_of_target_volume[0]\n",
    "    y_start = int(y / 2) - int(shape_of_target_volume[1] / 2)\n",
    "    y_end = y_start + shape_of_target_volume[1]\n",
    "    z_start = int(z / 2) - int(shape_of_target_volume[2] / 2)\n",
    "    z_end = z_start + shape_of_target_volume[2]\n",
    "\n",
    "    array_intermediate = array_intermediate[x_start:x_end, y_start:y_end, z_start:z_end]  # Now the array is padded\n",
    "\n",
    "    # rescaling:\n",
    "    array_standard_xy = np.zeros((target_shape[0], target_shape[1], shape_of_target_volume[2]), 'float32')\n",
    "    for s in range(shape_of_target_volume[2]):\n",
    "        array_standard_xy[:, :, s] = cv2.resize(array_intermediate[:, :, s], (target_shape[0], target_shape[1]), cv2.INTER_LANCZOS4)\n",
    "\n",
    "    array_standard = np.zeros(target_shape, 'float32')\n",
    "    for s in range(target_shape[0]):\n",
    "        array_standard[s, :, :] = cv2.resize(array_standard_xy[s, :, :], (target_shape[1], target_shape[2]), cv2.INTER_LINEAR)\n",
    "\n",
    "    return array_standard\n",
    "\n",
    "def normalize_range(array, lower, upper):\n",
    "    \"\"\"\n",
    "    Normalizes input array in a range.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        array: numpy array, \n",
    "            input array to be normalized.\n",
    "        lower: float, \n",
    "            lower bound of normalization.\n",
    "        upper: float, \n",
    "            upper bound of normalization.\n",
    "    Returns:\n",
    "        A numpy array with normalized values between lower and upper.\n",
    "    \"\"\"\n",
    "    newarray = array\n",
    "\n",
    "    newarray += (0 - np.min(newarray))\n",
    "    if np.amax(newarray) == 0:\n",
    "        return np.full(array.shape, lower)\n",
    "    else:\n",
    "        newarray *= ((upper - lower) / np.amax(newarray))\n",
    "        newarray += lower\n",
    "        \n",
    "    return newarray\n",
    "\n",
    "def normalize_mean_std(array, mean, std):   \n",
    "    \"\"\"\n",
    "    Normalizes input array with mean and standard deviation.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        array: numpy array, \n",
    "            input array to be normalized.\n",
    "        mean: float, \n",
    "            mean of the target distribution.\n",
    "        std: float, \n",
    "            standard deviation of the target distribution.\n",
    "    Returns:\n",
    "        A numpy array with normalized values.\n",
    "    \"\"\"\n",
    " \n",
    "    return (array-mean)/std\n",
    "\n",
    "\n",
    "def normalization_custom(img, apply_histeq=True, apply_clahe=True, adjust_gamma=True, gamma_coeff=0.6):\n",
    "    \"\"\"\n",
    "    Normalization test: normalize the input 2D image with a custom normalization technique.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        img: numpy array,\n",
    "            the image to be normalized\n",
    "        apply_histeq: bool, Optional,\n",
    "            apply or not the histogram equalization\n",
    "        apply_clahe: bool, Optional,\n",
    "            apply or not the CLAHE\n",
    "        adjust_gamma: bool, Optional,\n",
    "            adjust or not the gamma\n",
    "        gamma_coeff: float, Optional,\n",
    "            gamma adjustment coefficient\n",
    "    Returns:\n",
    "        The normalized input as numpy array inside the interval [-0.5, 0.5]\n",
    "    \"\"\"\n",
    "    if np.min(img) == np.max(img): # if the image is empty return all zeros\n",
    "        return np.zeros(img.shape)\n",
    "    out = normalize_range(img, -1250, 250) # bring HU in a sweet intervall for lung cts\n",
    "    if apply_histeq:\n",
    "        out = histeq(out, coeff=10) # equalize histogram in the intervall [-1250, 250]\n",
    "    out = paper_normalization(out) # apply the normalization presented in the original paper\n",
    "    out += 0.5 # bring in [0,1]\n",
    "    if adjust_gamma:\n",
    "        out = exposure.adjust_gamma(out, gamma_coeff)   \n",
    "    if apply_clahe:\n",
    "        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8)) # clahe to enhance features\n",
    "        out = (out*255).astype(np.uint8) # for clahe images must be in [0, 255]\n",
    "        out = clahe.apply(out) # apply clahe\n",
    "        out = out.astype(np.float32) / 255 # bring images back to [0,1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def paper_normalization(img, in_range=None): \n",
    "    \"\"\"\n",
    "    Normalize the input 2D image as suggested by a paper, sees the report for more details.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        img: numpy array,\n",
    "            the image to be normalized\n",
    "        in_range: str, Optional,\n",
    "            shifts to the interval [0, 1] if '0_1', [0, 255] if '0_255', keeps in [-0.5, 0.5] if None\n",
    "    Returns:\n",
    "        The normalized input as numpy array\n",
    "    \"\"\"\n",
    "    data_array = np.array(img)\n",
    "    min, max = np.min(img), np.max(img)\n",
    "    ww = abs(min) + abs(max)\n",
    "    if ww == 0:\n",
    "        return np.full(img.shape, -0.5)\n",
    "    wc = (min + max)/2\n",
    "    data_array = data_array - wc\n",
    "    data_array = data_array / ww   \n",
    "    if in_range == '0_1':\n",
    "        data_array += 0.5\n",
    "    elif in_range == '0_255':\n",
    "        data_array = (data_array + 0.5) * 255\n",
    "    return data_array\n",
    "\n",
    "def normalization_best(img):\n",
    "    \"\"\"\n",
    "    Normalizes the input 2D image. Inspired by paper_normalization, sees the report for more details.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        img: numpy array,\n",
    "            the image to be normalized\n",
    "    Returns:\n",
    "        The normalized input as numpy array inside the interval [0, 255]\n",
    "    \"\"\"\n",
    "    data_array = normalize_range(img, -1250, 250)\n",
    "    min, max = np.min(img), np.max(img)\n",
    "    ww = abs(min) + abs(max)\n",
    "    if ww == 0:\n",
    "        return np.full(img.shape, 0)\n",
    "    wc = (min + max)/2\n",
    "    data_array = data_array - wc\n",
    "    data_array = data_array / ww\n",
    "    data_array += 0.5\n",
    "    data_array *= 255\n",
    "    return data_array \n",
    "\n",
    "def comp_h(im, n_bins):\n",
    "    \"\"\"\n",
    "    Computes histogram\n",
    "    -----------\n",
    "    Parameters:\n",
    "        im: uint8 numpy array,\n",
    "            image of which the histogram has to be computed\n",
    "        n_bins: int,\n",
    "            number of bins\n",
    "    Returns:\n",
    "        The image histogram\n",
    "    \"\"\"\n",
    "    s1, s2 = im.shape\n",
    "    h = [0.0] * (n_bins+1)\n",
    "    for i in range(s1):\n",
    "        for j in range(s2):\n",
    "            h[im[i, j]]+=1\n",
    "    return np.array(h)\n",
    "\n",
    "def histeq(image, coeff=100):\n",
    "    \"\"\"\n",
    "    Histogram equalization for images in ranges different from [0,255]\n",
    "    -----------\n",
    "    Parameters:\n",
    "        image: numpy array,\n",
    "            the image to be equalized\n",
    "        coeff: int, Optional,\n",
    "            coefficient multiplied to the image to keep decimal approximations\n",
    "    Returns:\n",
    "        The equalized image\n",
    "    \"\"\"\n",
    "    image *= coeff\n",
    "    prev_min = np.abs(np.min(image))\n",
    "    image += prev_min\n",
    "    image = image.astype(np.uint32)\n",
    "    n_bins = np.max(image)\n",
    "    h = comp_h(image, n_bins)\n",
    "    s1, s2 = image.shape\n",
    "    cdf = np.cumsum(h)\n",
    "    cdf_min = np.amin(cdf)\n",
    "    val = (cdf-cdf_min)/((s1*s2)-cdf_min)\n",
    "    y = np.uint32(n_bins * val)\n",
    "    Y = np.zeros_like(image)\n",
    "    for i in range(0, s1): \n",
    "        for j in range(0, s2):\n",
    "            Y[i, j] = y[image[i, j]]\n",
    "    Y = Y.astype(np.float32) - prev_min\n",
    "    Y /= coeff\n",
    "    return Y\n",
    "\n",
    "def equalize(x, mode):\n",
    "    \"\"\"\n",
    "    Equalizes the input 2D image.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        x: uint8 numpy array,\n",
    "            the image to be equalized\n",
    "        mode: str,\n",
    "            the equalization method\n",
    "    Returns:\n",
    "        The equalized images as uin8 numpy array\n",
    "    \"\"\"\n",
    "    if mode=='histeq':\n",
    "        out = cv2.equalizeHist(x)\n",
    "    elif mode == 'clahe':\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        out = clahe.apply(x)\n",
    "    elif mode == 'clahe+histeq':\n",
    "        # CLAHE\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        out = clahe.apply(x)\n",
    "        # Hist eq\n",
    "        out = cv2.equalizeHist(out)\n",
    "    elif mode == 'histeq+clahe':\n",
    "        # Hist eq\n",
    "        out = cv2.equalizeHist(x)\n",
    "        # CLAHE\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        out = clahe.apply(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------NORMALIZATION APPROACH---------------------------------------------------------\n",
    "\n",
    "# Normalization and Equalization Handler\n",
    "def norm_and_eq(x, mean=None, std=None):\n",
    "    if norm_method == 'None':\n",
    "        out = np.array(x)\n",
    "    elif norm_method == 'custom':\n",
    "        out = normalization_custom(x, apply_histeq=True, apply_clahe=True, adjust_gamma=True, gamma_coeff=0.6)\n",
    "    elif norm_method == 'mean_std':\n",
    "        out = normalize_mean_std(x, mean, std)\n",
    "    elif norm_method == 'as_paper':\n",
    "        out = paper_normalization(x)\n",
    "    elif norm_method == 'as_paper_in_0_1':\n",
    "        out = paper_normalization(x, in_range='0_1')\n",
    "    elif norm_method == 'as_paper_in_0_255':\n",
    "        out = paper_normalization(x, in_range='0_255')\n",
    "    elif norm_method == 'in_range':\n",
    "        out = normalize_range(x, lower, higher)\n",
    "    elif norm_method == 'adjust_gamma':\n",
    "        out = normalize_range(x, 0, 1)\n",
    "        out = exposure.adjust_gamma(out, 0.6)   \n",
    "    elif norm_method == 'sharp':\n",
    "        out = normalize_range(x, 0, 255).astype(np.uint8)\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        out = cv2.filter2D(out, -1, kernel)\n",
    "        out = equalize(out, 'clahe+histeq').astype(np.float32)/255\n",
    "    elif norm_method == 'best' or norm_method == 'as_colab':\n",
    "        out = normalization_best(x)     \n",
    "    if eq_method != 'None':\n",
    "        out = equalize(out.astype(np.uint8), eq_method).astype(np.float32)/255\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_one_ct(scan):\n",
    "    \"\"\"\n",
    "    Rescales the input .nii.gz ct-scan volume.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        scan: nibabel object,\n",
    "            the ct-scan\n",
    "    Returns:\n",
    "        The rescaled ct-scan volume as numpy array\n",
    "    \"\"\"    \n",
    "    resolution = scan.header.get_zooms()\n",
    "    return rescale_to_standard(scan.get_fdata(), resolution, target_resolution, target_shape)\n",
    "\n",
    "def rescale_one_gt(scan):\n",
    "    \"\"\"\n",
    "    Rescales the input .nii.gz mask volume.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        scan: nibabel object,\n",
    "            the ct-scan mask\n",
    "    Returns:\n",
    "        The rescaled mask volume as numpy array\n",
    "    \"\"\"    \n",
    "    resolution = scan.header.get_zooms()\n",
    "    mask = rescale_to_standard(scan.get_fdata(), resolution, target_resolution, target_shape)\n",
    "    mask = mask.round()\n",
    "    return mask\n",
    "\n",
    "def preprocess_one_patient(ct_scan, mask):\n",
    "    \"\"\"\n",
    "    Preprocess one patient by rescaling the ct-scan and relative mask.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        ct_scan: nibabel object,\n",
    "            the ct-scan,\n",
    "        mask: nibabel_object,\n",
    "            the ct-scan mask\n",
    "    Returns:\n",
    "        The rescaled ct-scan volume and relative mask as numpy arrays\n",
    "    \"\"\"    \n",
    "    if isinstance(ct_scan, str):\n",
    "        ct_scan = nib.load(ct_scan)\n",
    "    if isinstance(mask, str):\n",
    "        mask = nib.load(mask)\n",
    "    # Process CT\n",
    "    ct = rescale_one_ct(ct_scan)\n",
    "    # Process GT\n",
    "    gt = rescale_one_gt(mask)\n",
    "    return ct, gt\n",
    "\n",
    "def preprocess_all(dataset, ct_dst, gt_dst, n_of_patients=-1):\n",
    "    \"\"\"\n",
    "    Preprocess all the patients inside a dict by rescaling the ct-scan and relative mask, \n",
    "    equalizing the ct-scan and saving each slice as .npy\n",
    "    -----------\n",
    "    Parameters:\n",
    "        dataset: dict,\n",
    "            dict with all the dataset paths\n",
    "        ct_dst: str,\n",
    "            save path for ct-scans\n",
    "        gt_dst: str,\n",
    "            save path for masks\n",
    "        n_of_patients: int, Optional,\n",
    "            number of patients to be processed, default value -1 processes all the patients\n",
    "    \"\"\"   \n",
    "    # Prepare paths\n",
    "    for split in ['training', 'validation']:\n",
    "        all_ct_paths = dataset['image'][split]\n",
    "        all_ct_paths.sort(key=natural_keys)\n",
    "        all_gt_paths = dataset['mask'][split]\n",
    "        all_gt_paths.sort(key=natural_keys)\n",
    "        # Check couples ct-gt\n",
    "        start_time = time.time()\n",
    "        for ct_path, gt_path in zip(all_ct_paths, all_gt_paths):\n",
    "            if ct_path.split(\"/\")[-1] != gt_path.split(\"/\")[-1]:\n",
    "                print(\"CT and GT pahts have different filenames inside\")\n",
    "                break  \n",
    "        # Preprocess   \n",
    "        iter = 1\n",
    "        for ct_path, gt_path in zip(all_ct_paths, all_gt_paths):\n",
    "            patient_time = time.time()\n",
    "            ct, gt = preprocess_one_patient(ct_path, gt_path)\n",
    "            new_ct_path = ct_dst + split + '/' \n",
    "            new_gt_path = gt_dst + split + '/' \n",
    "            save_data(ct, new_ct_path, ct_path.split(\"/\")[-1].split(\".\")[0])\n",
    "            save_data(gt, new_gt_path, gt_path.split(\"/\")[-1].split(\".\")[0])\n",
    "            iter+=1\n",
    "            if iter == n_of_patients:\n",
    "                break\n",
    "            print(\"{} \\t Done in {}\".format(ct_path.split(\"/\")[-1], str(datetime.timedelta(seconds=time.time()-patient_time)).split('.')[0]))\n",
    "    print('Dataset processed in {}'.format(str(datetime.timedelta(seconds=time.time()-start_time)).split('.')[0]))\n",
    "\n",
    "def save_data(img, dst, name, histeq=None):\n",
    "    \"\"\"\n",
    "    Apply equalization and save each volume's slice as .npy to the destination folder.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        img: numpy array,\n",
    "            volume to be processed\n",
    "        dst: str,\n",
    "            destination save path\n",
    "        name: str,\n",
    "            name of the folder containing all the slices\n",
    "        histeq: str, Optional,\n",
    "            histogram equalization method to be applied\n",
    "    \"\"\"\n",
    "    savepath = dst + name +'/'\n",
    "    os.makedirs(savepath + 'x/', exist_ok=True)\n",
    "    os.makedirs(savepath + 'y/', exist_ok=True)\n",
    "    os.makedirs(savepath + 'z/', exist_ok=True)\n",
    "    for idx in range(img.shape[0]):\n",
    "        x_slice = img[idx, :, :]\n",
    "        y_slice = img[:, idx, :]\n",
    "        z_slice = img[:, :, idx]\n",
    "        np.save(savepath + 'x/' + str(idx) + '.npy', x_slice)\n",
    "        np.save(savepath + 'y/' + str(idx) + '.npy', y_slice)\n",
    "        np.save(savepath + 'z/' + str(idx) + '.npy', z_slice)\n",
    "        \n",
    "def prepare_dirs(root):\n",
    "    \"\"\"\n",
    "    Prepare directories to store images for training and validation\n",
    "    -----------\n",
    "    Parameters:\n",
    "        root: str,\n",
    "            main folder path\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(root):\n",
    "        for folder in ['image/', 'mask/']:\n",
    "            for subfolder in ['training/', 'validation/']:\n",
    "                os.makedirs(root + folder + subfolder, exist_ok=True)          \n",
    "\n",
    "# If the flag is True, rescale, equalize and store as .npy the datasets\n",
    "if preprocess_dataset:\n",
    "    print(\"Processing Zenodo\")\n",
    "    prepare_dirs(zenodo_root_proc)\n",
    "    preprocess_all(zenodo, zenodo_image_root_proc, zenodo_mask_root_proc)\n",
    "    print(\"\\n\\nProcessing Challenge\")\n",
    "    prepare_dirs(challenge_root_proc)\n",
    "    preprocess_all(challenge, challenge_image_root_proc, challenge_mask_root_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igH9716cKtHP"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-BSUMahYDcd0"
   },
   "outputs": [],
   "source": [
    "class orthogonal_rotation():\n",
    "    \"\"\"\n",
    "    Class to apply orthogonal rotaton to images with a random angle picked from a list of int\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.angles = [90, 180, 270]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Augmentation technique, rotate with a random angle picked from a list of int\n",
    "        -----------\n",
    "        Parameters:\n",
    "            x: Tensor,\n",
    "                image tensor to be augmented\n",
    "        Returns:\n",
    "            The rotated image tensor\n",
    "        \"\"\"\n",
    "        angle = random.choice(self.angles)\n",
    "        return transforms.functional.rotate(x, angle)\n",
    "    \n",
    "class COVID_dataset():\n",
    "    \"\"\"\n",
    "    Class to handle datasets\n",
    "    -----------\n",
    "    Parameters:\n",
    "        dataset: dict or list of dict,\n",
    "            dataset dict with all the relative paths\n",
    "        dts_type: str or list of str,\n",
    "            defining the dataset split folder, 'training' or 'validation'\n",
    "        aug: bool,\n",
    "            apply or not augmentation\n",
    "        shape: tuple,\n",
    "            images target shape\n",
    "        size: int,\n",
    "            number of samples to work with\n",
    "    \"\"\"   \n",
    "    def __init__(self, dataset, dts_type, aug, shape, size=None):\n",
    "        self.dataset = dataset\n",
    "        self.dts_type = dts_type\n",
    "        self.aug = aug\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.shape = shape\n",
    "        self.mean = 0\n",
    "        self.std = 0\n",
    "        # Handle unique dataset\n",
    "        if not isinstance(self.dataset, list):\n",
    "            for idx in range(len(self.dataset['image'][self.dts_type])):\n",
    "                for axis in ('/x/', '/y/', '/z/'):\n",
    "                    if axis == '/x/':\n",
    "                        n_samp = self.shape[0]\n",
    "                    elif axis == '/y/':\n",
    "                        n_samp = self.shape[1]\n",
    "                    elif axis == '/z/':\n",
    "                        n_samp = self.shape[2]   \n",
    "                    for slice_n in range(n_samp):\n",
    "                        self.x.append(self.dataset['image'][self.dts_type][idx] + axis + str(slice_n) + '.npy')\n",
    "                        self.y.append(self.dataset['mask'][self.dts_type][idx] + axis + str(slice_n) + '.npy')   \n",
    "            self.mean += self.dataset['mean']\n",
    "            self.std += self.dataset['std']\n",
    "                        \n",
    "        # Handle more datasets\n",
    "        else:\n",
    "            for subset in self.dataset:\n",
    "                for idx in range(len(subset['image'][self.dts_type])):\n",
    "                    for axis in ('/x/', '/y/', '/z/'):\n",
    "                        if axis == '/x/':\n",
    "                            n_samp = self.shape[0]\n",
    "                        elif axis == '/y/':\n",
    "                            n_samp = self.shape[1]\n",
    "                        elif axis == '/z/':\n",
    "                            n_samp = self.shape[2]   \n",
    "                        for slice_n in range(n_samp):\n",
    "                            self.x.append(subset['image'][self.dts_type][idx] + axis + str(slice_n) + '.npy')\n",
    "                            self.y.append(subset['mask'][self.dts_type][idx] + axis + str(slice_n) + '.npy')  \n",
    "                self.mean += subset['mean']\n",
    "                self.std += subset['std']\n",
    "            self.mean /= len(self.dataset)\n",
    "            self.std /= len(self.dataset)\n",
    "\n",
    "\n",
    "        if len(self.x)!=len(self.y): raise SystemError('Problem with Img and Gt, no same size')\n",
    "            \n",
    "        self.x.sort(key=natural_keys)\n",
    "        self.y.sort(key=natural_keys)\n",
    "        \n",
    "        if size is not None:\n",
    "            self.x = self.x[:size]\n",
    "            self.y = self.y[:size]\n",
    "\n",
    "        self.info = len(self.x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.info\n",
    "  \n",
    "\n",
    "    def augmentation2D(self, scan, mask):\n",
    "        \"\"\"\n",
    "        Apply augmentation\n",
    "        -----------\n",
    "        Parameters:\n",
    "            scan: Tensor,\n",
    "                ct-scan tensor\n",
    "            mask: Tensor,\n",
    "                mask tensor\n",
    "        Returns:\n",
    "            the augmented ct and mask tensors\n",
    "        \"\"\"  \n",
    "        #methods\n",
    "        rand_crop = transforms.RandomResizedCrop(512, scale=(0.5, 0.5), ratio=(1.0, 1.0), interpolation=transforms.InterpolationMode.NEAREST) \n",
    "        rand_ho_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "        rand_ve_flip = transforms.RandomVerticalFlip(p=1.0)\n",
    "        rand_rotate = transforms.RandomRotation(5, interpolation=transforms.InterpolationMode.NEAREST, expand=False, center=None, fill=0)\n",
    "        rand_orth_rotate = orthogonal_rotation()\n",
    "        \n",
    "        if torch.sum(mask) == 0:\n",
    "            return scan, mask\n",
    "        else: \n",
    "            w,h = scan.shape[1:]\n",
    "            output = torch.zeros([2,w,h])\n",
    "            output[0,:,:] = scan\n",
    "            output[1,:,:] = mask\n",
    "            aug_method = np.random.randint(0,5)\n",
    "            # random horizontal flip\n",
    "            if aug_method == 0:\n",
    "                output = rand_ho_flip(output)\n",
    "            # random vertical flip\n",
    "            elif aug_method == 1:\n",
    "                output = rand_ve_flip(output)\n",
    "            # random crop\n",
    "            elif aug_method == 2:\n",
    "                output = rand_crop(output)\n",
    "            # random rotate\n",
    "            elif aug_method == 3:\n",
    "                output = rand_rotate(output)\n",
    "            # random orthogonal rotate\n",
    "            elif aug_method == 4:\n",
    "                output = rand_orth_rotate(output)\n",
    "\n",
    "            scan_output = output[0,:,:].unsqueeze(0)\n",
    "            mask_output = output[1,:,:].unsqueeze(0)\n",
    "\n",
    "            return scan_output, mask_output\n",
    "        \n",
    "    \n",
    "    def __load_nii__(self, filepath):\n",
    "        return nib.load(filepath)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index=None):    \n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.info)\n",
    "        \n",
    "        # Load Image\n",
    "        ct_scan = np.load(self.x[index])\n",
    "        mask = np.load(self.y[index]) \n",
    "        \n",
    "        # Normalize and Equalize\n",
    "        ct_scan = norm_and_eq(ct_scan, self.mean, self.std)\n",
    "\n",
    "        ct_scan = torch.from_numpy(ct_scan).unsqueeze(0)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "    \n",
    "        # Augmentation\n",
    "        if self.aug:\n",
    "            ct_scan, mask = self.augmentation2D(ct_scan, mask)\n",
    "        \n",
    "        return ct_scan.float(), mask.float()\n",
    "\n",
    "    def get_file(self, filename):    \n",
    "        index = [i for i, s in enumerate(self.x) if filename == s][0]\n",
    "        return self.__getitem__(index)\n",
    "    \n",
    "\n",
    "def init_train_test_loader(train_dataset, train_folder, valid_dataset, valid_folder, num_workers, size_train=None, size_valid=None):\n",
    "    \"\"\"\n",
    "    Create data loaders for training and validation splits\n",
    "    -----------\n",
    "    Parameters:\n",
    "        train_dataset: dict or list of dict,\n",
    "            dataset dict with all the relative paths used for training\n",
    "        train_folder: str or list of str,\n",
    "            defining the dataset split folder, 'training' or 'validation'\n",
    "        valid_dataset: dict or list of dict,\n",
    "            dataset dict with all the relative paths used for validation\n",
    "        valid_folder: str or list of str,\n",
    "            defining the dataset split folder, 'training' or 'validation'\n",
    "        num_workers: int,\n",
    "            number of workers for each data loader\n",
    "        size_train: int, Optional,\n",
    "            number of samples for the training split\n",
    "        size_valid: int, Optional,\n",
    "            number of samples for the validation split\n",
    "    Returns:\n",
    "        Dataloaders for both training and validation\n",
    "    \"\"\"\n",
    "    # Training Data loader\n",
    "    training_Dataset = COVID_dataset(train_dataset, train_folder, aug=True, shape=target_shape, size=size_train)\n",
    "    training_DataLoader = DataLoader(training_Dataset, batch_size=training_batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    # Test Data loader\n",
    "    test_Dataset = COVID_dataset(valid_dataset, valid_folder, aug=False, shape=target_shape, size=size_valid)\n",
    "    test_DataLoader = DataLoader(test_Dataset, batch_size=validation_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return training_DataLoader, test_DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL2hgHiGOIIy"
   },
   "source": [
    "## Test data correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Vm8BRViROIkW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZENODO\n",
      "\n",
      "\n",
      "\n",
      "CHALLENGE\n"
     ]
    }
   ],
   "source": [
    "def dataset_processed_test(dataset_proc, dataset_root, filename=None):\n",
    "    \"\"\"\n",
    "    Quick test to check dataset format\n",
    "    -----------\n",
    "    Parameters:\n",
    "        dataset_proc: dict,\n",
    "            processed dataset dict with all the relative paths\n",
    "        dataset_root: dict\n",
    "            original dataset dict with all the relative paths\n",
    "        filename: str, Optional,\n",
    "            dataset sample on which perform the test, if None it is randomly picked\n",
    "    \"\"\"\n",
    "    # nii.gz\n",
    "    if filename == None:\n",
    "        idx = np.random.randint(0, len(dataset_proc['image']['training']))\n",
    "        filename = dataset_proc['image']['training'][idx]\n",
    "        filename_gt = dataset_proc['mask']['training'][idx]\n",
    "    \n",
    "    slice_id = target_shape[2] // 2\n",
    "    \n",
    "    scan = load_scan(dataset_root + 'image/training/' + filename.split(\"/\")[-1] + '.nii.gz')\n",
    "    mask = load_scan(dataset_root + 'mask/training/' + filename_gt.split(\"/\")[-1] + '.nii.gz')\n",
    "    \n",
    "    # Original slice\n",
    "    slice_mask = slice_image(mask, mask.shape[2]//2)\n",
    "    slice_scan = slice_image(scan, scan.shape[2]//2)\n",
    "    # Rescale and normalization\n",
    "    scan_res, mask_res = preprocess_one_patient(scan, mask)\n",
    "    slice_mask_res = mask_res[:,:,slice_id]\n",
    "    slice_scan_res = scan_res[:,:,slice_id]\n",
    "\n",
    "    # Dataloader\n",
    "    training_Dataset = COVID_dataset(dataset_proc, 'training', aug=True, shape=target_shape)\n",
    "    scan_aug, mask_aug = training_Dataset.get_file(filename=filename + '/z/256.npy')\n",
    "    scan_aug = scan_aug.squeeze().numpy()\n",
    "    mask_aug = mask_aug.squeeze().numpy()     \n",
    "\n",
    "    images = [slice_scan, slice_scan_res, scan_aug, slice_mask, slice_mask_res, mask_aug]\n",
    "    \n",
    "    f, axarr = plt.subplots(1, 6, figsize=(20,20))\n",
    "    for axis, i in zip(['CT-scan', 'CT Preprocessed', 'CT Augmented', 'Mask slice', 'Mask Normalized', 'Mask augmented'], range(6)):\n",
    "        axarr[i].imshow(images[i], cmap='gray')\n",
    "        axarr[i].set_xlabel(axis)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    mask_values = np.unique(mask_aug)\n",
    "    scan_min_check = 'OK' if np.min(scan_aug) >= 0 else 'NO'\n",
    "    scan_max_check = 'OK' if (np.max(scan_aug) <= 1 and np.max(scan_aug) > 0) else 'NO'\n",
    "    mask_min_check = 'OK' if np.min(mask_aug) == 0 else 'NO'\n",
    "    mask_max_check = 'OK' if np.max(mask_aug) == 2  else 'NO'\n",
    "    n_val_mask = 'OK' if len(mask_values) == 3 else 'NO'\n",
    "\n",
    "    print('\\nTest for DataLoader')\n",
    "    print('Check:\\nAugmented ct-scan:\\n\\t\\t Expected min possible val: 0\\t Actual min val: {}\\t{}\\n\\t\\t Expected max possible val: 1\\t Actual max val: {:.2f}\\t{}\\n'.format(np.min(scan_aug), scan_min_check, np.max(scan_aug), scan_max_check))\n",
    "    print('Check:\\nAugmented mask:\\n\\t\\t Expected min possible val: 0\\t Actual min val: {}\\t{}\\n\\t\\t Expected max possible val: 2\\t Actual max val: {}\\t{}'.format(np.min(mask_aug), mask_min_check, np.max(mask_aug), mask_max_check))\n",
    "    print('Check:\\n\\t\\tPossible mask values: 3\\t\\t Actual val: {}\\t{}'.format(mask_values,n_val_mask))\n",
    "    \n",
    "print('ZENODO')\n",
    "#dataset_processed_test(zenodo_proc, zenodo_root)\n",
    "print('\\n\\n\\nCHALLENGE')\n",
    "#dataset_processed_test(challenge_proc, challenge_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKhgcipHGfHD"
   },
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q7tLZXzvWN9"
   },
   "source": [
    "## TransUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU8dMuV01Lya"
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-fVUYzsT1NUS"
   },
   "outputs": [],
   "source": [
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "\n",
    "    config.classifier = 'seg'\n",
    "    config.representation_size = None\n",
    "    config.resnet_pretrained_path = None\n",
    "    config.pretrained_path = 'models/vit_checkpoint/imagenet21k+imagenet2012/ViT-B_16.npz'\n",
    "    config.patch_size = 16\n",
    "\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.n_classes = n_classes\n",
    "    config.activation = 'softmax'\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_testing():\n",
    "    \"\"\"Returns a minimal configuration for testing.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 1\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 1\n",
    "    config.transformer.num_heads = 1\n",
    "    config.transformer.num_layers = 1\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    return config\n",
    "\n",
    "def get_r50_b16_config():\n",
    "    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n",
    "    config = get_b16_config()\n",
    "    config.patches.grid = (16, 16)\n",
    "    config.resnet = ml_collections.ConfigDict()\n",
    "    config.resnet.num_layers = (3, 4, 9)\n",
    "    config.resnet.width_factor = 1\n",
    "\n",
    "    config.classifier = 'seg'\n",
    "    config.pretrained_path = 'models/vit_checkpoint/imagenet21k+imagenet2012/R50+ViT-B_16.npz'\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.skip_channels = [512, 256, 64, 16]\n",
    "    config.n_classes = n_classes\n",
    "    config.n_skip = 3\n",
    "    config.activation = 'softmax'\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_b32_config():\n",
    "    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n",
    "    config = get_b16_config()\n",
    "    config.patches.size = (32, 32)\n",
    "    config.pretrained_path = 'models/vit_checkpoint/imagenet21k+imagenet2012/ViT-B_32.npz'\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_l16_config():\n",
    "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 1024\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 4096\n",
    "    config.transformer.num_heads = 16\n",
    "    config.transformer.num_layers = 24\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.representation_size = None\n",
    "\n",
    "    # custom\n",
    "    config.classifier = 'seg'\n",
    "    config.resnet_pretrained_path = None\n",
    "    config.pretrained_path = 'models/vit_checkpoint/imagenet21k+imagenet2012/ViT-L_16.npz'\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.n_classes = n_classes\n",
    "    config.activation = 'softmax'\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_l32_config():\n",
    "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
    "    config = get_l16_config()\n",
    "    config.patches.size = (32, 32)\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_h14_config():\n",
    "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n",
    "    config.hidden_size = 1280\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 5120\n",
    "    config.transformer.num_heads = 16\n",
    "    config.transformer.num_layers = 32\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "\n",
    "    return config\n",
    "\n",
    "CONFIGS = {\n",
    "    'ViT-B_16': get_b16_config(),\n",
    "    'ViT-B_32': get_b32_config(),\n",
    "    'ViT-L_16': get_l16_config(),\n",
    "    'ViT-L_32': get_l32_config(),\n",
    "    'ViT-H_14': get_h14_config(),\n",
    "    'R50+ViT-B_16': get_r50_b16_config(),\n",
    "    'testing': get_testing(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi2jcgQC1aYz"
   },
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lk58sMi_vXyS"
   },
   "outputs": [],
   "source": [
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class StdConv2d(nn.Conv2d):\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
    "                        self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=bias, groups=groups)\n",
    "\n",
    "\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation (v2) bottleneck block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "        super().__init__()\n",
    "        cout = cout or cin\n",
    "        cmid = cmid or cout//4\n",
    "\n",
    "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n",
    "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
    "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if (stride != 1 or cin != cout):\n",
    "            # Projection also with pre-activation according to paper.\n",
    "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
    "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.gn_proj(residual)\n",
    "\n",
    "        # Unit's branch\n",
    "        y = self.relu(self.gn1(self.conv1(x)))\n",
    "        y = self.relu(self.gn2(self.conv2(y)))\n",
    "        y = self.gn3(self.conv3(y))\n",
    "\n",
    "        y = self.relu(residual + y)\n",
    "        return y\n",
    "\n",
    "    def load_from(self, weights, n_block, n_unit):\n",
    "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
    "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
    "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
    "\n",
    "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
    "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
    "\n",
    "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
    "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
    "\n",
    "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
    "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
    "\n",
    "        self.conv1.weight.copy_(conv1_weight)\n",
    "        self.conv2.weight.copy_(conv2_weight)\n",
    "        self.conv3.weight.copy_(conv3_weight)\n",
    "\n",
    "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
    "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
    "\n",
    "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
    "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
    "\n",
    "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
    "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
    "\n",
    "        if hasattr(self, 'downsample'):\n",
    "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
    "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
    "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
    "\n",
    "            self.downsample.weight.copy_(proj_conv_weight)\n",
    "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
    "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n",
    "\n",
    "class ResNetV2(nn.Module):\n",
    "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
    "\n",
    "    def __init__(self, block_units, width_factor):\n",
    "        super().__init__()\n",
    "        width = int(64 * width_factor)\n",
    "        self.width = width\n",
    "\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
    "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
    "                ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
    "                ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
    "                ))),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        b, c, in_size, _ = x.size()\n",
    "        x = self.root(x)\n",
    "        features.append(x)\n",
    "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n",
    "        for i in range(len(self.body)-1):\n",
    "            x = self.body[i](x)\n",
    "            right_size = int(in_size / 4 / (i+1))\n",
    "            if x.size()[2] != right_size:\n",
    "                pad = right_size - x.size()[2]\n",
    "                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n",
    "                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n",
    "                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n",
    "            else:\n",
    "                feat = x\n",
    "            features.append(feat)\n",
    "        x = self.body[-1](x)\n",
    "        return x, features[::-1]\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = torch.nn.functional.gelu\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        self.config = config\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:   # ResNet\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n",
    "            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  \n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.hidden_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hybrid:\n",
    "            x, features = self.hybrid_model(x)\n",
    "        else:\n",
    "            features = None\n",
    "        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings, features\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/query\", \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/key\", \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/value\", \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/out\", \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/query\", \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/key\", \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/value\", \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, \"MultiHeadDotProductAttention_1/out\", \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, \"MlpBlock_3/Dense_0\", \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, \"MlpBlock_3/Dense_1\", \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, \"MlpBlock_3/Dense_0\", \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, \"MlpBlock_3/Dense_1\", \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, \"LayerNorm_0\", \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, \"LayerNorm_0\", \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, \"LayerNorm_2\", \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, \"LayerNorm_2\", \"bias\")]))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return encoded, attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output, features = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n",
    "        return encoded, attn_weights, features\n",
    "\n",
    "\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            skip_channels=0,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.up(x)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        super().__init__(conv2d, upsampling)\n",
    "\n",
    "\n",
    "class DecoderCup(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        head_channels = 512\n",
    "        self.conv_more = Conv2dReLU(\n",
    "            config.hidden_size,\n",
    "            head_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=True,\n",
    "        )\n",
    "        decoder_channels = config.decoder_channels\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        if self.config.n_skip != 0:\n",
    "            skip_channels = self.config.skip_channels\n",
    "            for i in range(4-self.config.n_skip):  # re-select the skip channels according to n_skip\n",
    "                skip_channels[3-i]=0\n",
    "\n",
    "        else:\n",
    "            skip_channels=[0,0,0,0]\n",
    "\n",
    "        blocks = [\n",
    "            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, hidden_states, features=None):\n",
    "        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, h, w)\n",
    "        x = self.conv_more(x)\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            if features is not None:\n",
    "                skip = features[i] if (i < self.config.n_skip) else None\n",
    "            else:\n",
    "                skip = None\n",
    "            x = decoder_block(x, skip=skip)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "        self.decoder = DecoderCup(config)\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=config['decoder_channels'][-1],\n",
    "            out_channels=config['n_classes'],\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n",
    "        x = self.decoder(x, features)\n",
    "        logits = self.segmentation_head(x)\n",
    "        return logits\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            res_weight = weights\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
    "\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            elif posemb.size()[1]-1 == posemb_new.size()[1]:\n",
    "                posemb = posemb[:, 1:]\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                ntok_new = posemb_new.size(1)\n",
    "                if self.classifier == \"seg\":\n",
    "                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = posemb_grid\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            # Encoder whole\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(res_weight, n_block=bname, n_unit=uname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IUHa2jz_voru"
   },
   "outputs": [],
   "source": [
    "def init_transunet(vit_name, n_skip, vit_patches_size, pretrained=True):\n",
    "    config_vit = CONFIGS[vit_name]\n",
    "    config_vit.n_classes = n_classes\n",
    "    config_vit.n_skip = n_skip\n",
    "    if vit_name.find('R50') != -1:\n",
    "        config_vit.patches.grid = (int(target_shape[0] / vit_patches_size), int( target_shape[1] / vit_patches_size))\n",
    "    model = VisionTransformer(config_vit, target_shape[0], num_classes=config_vit.n_classes).float().to(device)\n",
    "    if pretrained:\n",
    "        model.load_from(weights=np.load(config_vit.pretrained_path))\n",
    "    print(config_vit)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcmusTj9XeIT"
   },
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-oF3udvPXfgm"
   },
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "  \n",
    "def save_checkpoint(model, name):\n",
    "    \"\"\"\n",
    "    Saves a model\n",
    "    \"\"\"\n",
    "    if '_best' in name:\n",
    "        folder = name.split(\"_best\")[0]\n",
    "    elif '_checkpoint' in name:\n",
    "        folder = name.split(\"_checkpoint\")[0]\n",
    "    save_path = model_save_path + folder + '/'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path + name)\n",
    "    \n",
    "def load_model(model_config):\n",
    "    \"\"\"\n",
    "    Loads a model\n",
    "    -----------\n",
    "    Parameters:\n",
    "        model_config: dict,\n",
    "            dictionary with all the information needed about the model\n",
    "    Returns:\n",
    "        The loaded model and its name\n",
    "    \"\"\"\n",
    "    model_name = model_config['model_name'] + '_' + model_config['version']\n",
    "    if norm_method == 'in_range':\n",
    "        model_name = model_config['eq_method'] + '_' + model_config['norm_method'] +'['+str(model_config['lower'])+','+str(model_config['higher'])+']_' + model_name\n",
    "    else:\n",
    "        model_name = model_config['eq_method'] + '_' +  model_config['norm_method'] + '_' + model_name  \n",
    "    if model_config['batch_norm'] == 'inplace':\n",
    "        model_name = 'bn_inplace_' + model_name\n",
    "    if model_config['attention'] == 'scse':\n",
    "        model_name = 'scse_' + model_name\n",
    "    if model_config['fine_tuning']:\n",
    "        if not model_config['freeze_encoder']:\n",
    "            model_name = 'all_trainable_finetuning_' + model_name\n",
    "        else:\n",
    "            model_name = 'encoder_freezed_finetuning_' + model_name\n",
    "    model_name = model_config['prefix'] + model_name\n",
    "    model_folder = model_name.split('_'+model_config['version'])[0] +'/'\n",
    "   \n",
    "    # Init network\n",
    "    model, _ = init_model(model_config)\n",
    "    # Load model\n",
    "    print(\"\\nloading checkpoint for {}..\\n\".format(model_name))\n",
    "    model_dict = torch.load(model_config['weights_path'] + model_folder + model_name, map_location=torch.device(device))\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(\"checkpoint loaded\\n\")\n",
    "    return model, model_name\n",
    "\n",
    "# Model\n",
    "def init_model(model_config=None):\n",
    "    \"\"\"\n",
    "    Build a model\n",
    "    -----------\n",
    "    Parameters:\n",
    "        model_config: dict, Optional\n",
    "            if None build the model using global variables defined at the beggining,\n",
    "            otherwise it uses the model_config dictionary with all the information\n",
    "            needed about the model\n",
    "    Returns:\n",
    "        The built model and its name\n",
    "    \"\"\"\n",
    "    # Built for training using global variables\n",
    "    if model_config == None:\n",
    "        if approach == 'transformer':\n",
    "            model = init_transunet(vit_name, n_skip, vit_patches_size)\n",
    "            model_name = model_prefix_name + criterion.__class__.__name__ + '_' + model.__class__.__name__\n",
    "        elif approach == '2.5D':\n",
    "            if decoder_name == 'DeepLabV3':\n",
    "                model = smp.DeepLabV3(encoder_name=encoder_name, encoder_weights=encoder_weights, in_channels=1, classes=n_classes, activation=None, encoder_depth=5, decoder_channels=256)\n",
    "            elif decoder_name == 'MAnet':\n",
    "                model = smp.MAnet(encoder_name=encoder_name, encoder_weights=encoder_weights, in_channels=1, classes=n_classes, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
    "            elif decoder_name == 'Unet':\n",
    "                model = smp.Unet(encoder_name=encoder_name, encoder_weights=encoder_weights, in_channels=1, classes=n_classes, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16],\n",
    "                                 decoder_attention_type=attention, decoder_use_batchnorm=batch_norm)\n",
    "\n",
    "            if encoder_weights == None:\n",
    "                model_name = model_prefix_name + 'scratch_' + encoder_name + '_' + criterion.__class__.__name__ + '_' + model.__class__.__name__\n",
    "            else:\n",
    "                model_name = model_prefix_name + encoder_weights + '_' + encoder_name + '_' + criterion.__class__.__name__ + '_' + model.__class__.__name__\n",
    "    else:  \n",
    "        if model_config['approach'] == 'transformer':\n",
    "            model = init_transunet(model_config['vit_name'], model_config['n_skip'], model_config['vit_patches_size'])\n",
    "        elif model_config['approach'] == '2.5D':\n",
    "            if model_config['decoder_name'] == 'DeepLabV3':\n",
    "                model = smp.DeepLabV3(encoder_name=encoder_name, encoder_weights=encoder_weights, in_channels=1, classes=n_classes, activation=None, encoder_depth=5, decoder_channels=256)\n",
    "            elif model_config['decoder_name'] == 'MAnet':\n",
    "                model = smp.MAnet(encoder_name=encoder_name, encoder_weights=encoder_weights, in_channels=1, classes=n_classes, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
    "            elif model_config['decoder_name'] == 'Unet':\n",
    "                model = smp.Unet(encoder_name=model_config['encoder_name'], encoder_weights=model_config['encoder_weights'], in_channels=1, classes=n_classes, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16],\n",
    "                                decoder_attention_type=model_config['attention'], decoder_use_batchnorm=model_config['batch_norm'])\n",
    "        model_name = None\n",
    "    print(\"Model built\") \n",
    "    total_params = count_parameters(model)\n",
    "    trainable_params = count_trainable_parameters(model)\n",
    "    print('\\nTotal params: {}\\nTrainable params: {}\\nNon-Trainable params: {}'.format(total_params, trainable_params, (total_params - trainable_params)))\n",
    "\n",
    "    model.float().to(device)\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1soT4NouDLRg"
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JfLujcf16yus"
   },
   "outputs": [],
   "source": [
    "class Sobel(nn.Module):\n",
    "    \"\"\"\n",
    "    Mean Absolute Sobel Error class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Sobel, self).__init__()\n",
    "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "        edge_k = np.stack((edge_kx, edge_ky))\n",
    "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
    "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
    "\n",
    "        self.kernel = np.array([ [1, 1, 1],\n",
    "                  [1, 1, 1],\n",
    "                  [1, 1, 1] ], dtype=np.float32)\n",
    "        self.kernel_tensor = torch.Tensor(np.expand_dims(np.expand_dims(self.kernel, 0), 0)).to(device) # size: (1, 1, 3, 3)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        # Sobel preds\n",
    "        sobel_inputs = self.compute_sobel(inputs)\n",
    "        inputs_x = torch.clamp(torch.nn.functional.conv2d(sobel_inputs[:,0,:,:].unsqueeze(1), self.kernel_tensor, padding=(1, 1)), 0, 1)\n",
    "        inputs_y = torch.clamp(torch.nn.functional.conv2d(sobel_inputs[:,1,:,:].unsqueeze(1), self.kernel_tensor, padding=(1, 1)), 0, 1)\n",
    "        # Sobel gt\n",
    "        sobel_labels = self.compute_sobel(labels)\n",
    "        labels_x = torch.clamp(torch.nn.functional.conv2d(sobel_labels[:,0,:,:].unsqueeze(1), self.kernel_tensor, padding=(1, 1)), 0, 1)\n",
    "        labels_y = torch.clamp(torch.nn.functional.conv2d(sobel_labels[:,1,:,:].unsqueeze(1), self.kernel_tensor, padding=(1, 1)), 0, 1)\n",
    "\n",
    "        return torch.abs(inputs_x - labels_x).mean(), torch.abs(inputs_y - labels_y).mean()\n",
    "\n",
    "    def compute_sobel(self, x):\n",
    "        out = self.edge_conv(x) \n",
    "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
    "        return out\n",
    "\n",
    "\n",
    "def one_hot2D(labels, num_classes, device=None, dtype=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Convert BxHxW image tensor to one hot encoding image tensor with shape BxNxHxW with\n",
    "    B: batch size, H: height, W: width and N: number of classes\n",
    "    -----------\n",
    "    Parameters:\n",
    "        labels: Tensor,\n",
    "            BxHxW Tensor with the ground truth labels\n",
    "        num_classes: int,\n",
    "            total number of classes\n",
    "    Returns:\n",
    "        the one hot encoded tensor\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(labels):\n",
    "        raise TypeError(\"Input labels type is not a torch.Tensor. Got {}\"\n",
    "                      .format(type(labels)))\n",
    "    if not len(labels.shape) == 3:\n",
    "        raise ValueError(\"Invalid depth shape, we expect BxHxW. Got: {}\"\n",
    "                        .format(labels.shape))\n",
    "    if not labels.dtype == torch.int64:\n",
    "        raise ValueError(\n",
    "          \"labels must be of the same dtype torch.int64. Got: {}\" .format(\n",
    "              labels.dtype))\n",
    "    if num_classes < 1:\n",
    "        raise ValueError(\"The number of classes must be bigger than one.\"\n",
    "                        \" Got: {}\".format(num_classes))\n",
    "    batch_size, height, width = labels.shape\n",
    "    one_hot = torch.zeros(batch_size, num_classes, height, width,\n",
    "                        device=device, dtype=dtype)\n",
    "    return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps\n",
    "\n",
    "\n",
    "class DiceLoss2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss on 2D image tensor\n",
    "    -----------\n",
    "    Parameters:\n",
    "     weighted: bool,\n",
    "         Flag to apply or not weights to the labels\n",
    "    \"\"\"\n",
    "    def __init__(self, weighted) -> None:\n",
    "        super(DiceLoss2D, self).__init__()\n",
    "        self.eps: float = 1e-6\n",
    "        self.weighted = weighted\n",
    "        if self.weighted:\n",
    "            self.__class__.__name__ = 'WeightedDiceLoss'\n",
    "        else:\n",
    "            self.__class__.__name__ = 'DiceLoss'\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes dice loss\n",
    "        -----------\n",
    "        Parameters:\n",
    "            input: Tensor,\n",
    "                predictions tensor\n",
    "            target: Tensor,\n",
    "                ground truth masks tensor\n",
    "        Returns:\n",
    "            The dice loss value\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(input):\n",
    "            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                        .format(type(input)))\n",
    "        if not len(input.shape) == 4:\n",
    "            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
    "                          .format(input.shape))\n",
    "        if not input.shape[-2:] == target.shape[-2:]:\n",
    "            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
    "                          .format(input.shape, input.shape))\n",
    "        if not input.device == target.device:\n",
    "            raise ValueError(\n",
    "                \"input and target must be in the same device. Got: {}\" .format(\n",
    "                    input.device, target.device))\n",
    "        # compute softmax over the classes axis\n",
    "        input_soft = torch.nn.functional.softmax(input, dim=1)\n",
    "        input_soft = input_soft.view(input_soft.shape[0], n_classes, -1) # (B, Nclasses, HxW)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = one_hot2D(target, num_classes=input.shape[1],\n",
    "                                  device=input.device, dtype=input.dtype)\n",
    "        target_one_hot = target_one_hot.view(target_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxW)\n",
    "\n",
    "        # compute the actual dice score factors\n",
    "        intersection = torch.sum(input_soft * target_one_hot, 2)\n",
    "        cardinality = torch.sum(input_soft + target_one_hot, 2) # (B)\n",
    "\n",
    "        # count n element for each class\n",
    "        if self.weighted:\n",
    "            counts = torch.sum(target_one_hot, dim=2)\n",
    "            weights = torch.as_tensor(1. / (counts ** 2), dtype=float)\n",
    "            weights = torch.where(torch.isfinite(weights), weights, self.eps) # (B, Nclasses)\n",
    "            # apply weights\n",
    "            intersection = torch.sum(weights*intersection, axis=-1)\n",
    "            cardinality = torch.sum(weights*cardinality, axis=-1)\n",
    "            # compute dice score\n",
    "            dice_score = 1 - 2. * intersection / cardinality\n",
    "            dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "        else:\n",
    "            # compute dice score\n",
    "            dice_score = 1 - 2. * intersection / (cardinality + self.eps)\n",
    "            dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "\n",
    "        return torch.mean(dice_score)\n",
    "\n",
    "class DiceLoss2D_CE(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss on 2D image tensor with Cross Entropy Loss\n",
    "    -----------\n",
    "    Parameters:\n",
    "     weighted: bool,\n",
    "         Flag to apply or not weights to the labels\n",
    "    \"\"\"\n",
    "    def __init__(self, weighted) -> None:\n",
    "        super(DiceLoss2D_CE, self).__init__()\n",
    "        self.eps: float = 1e-6\n",
    "        self.weighted = weighted\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        if self.weighted:\n",
    "            self.__class__.__name__ = 'WeightedDiceLoss+CE'\n",
    "        else:\n",
    "            self.__class__.__name__ = 'DiceLoss+CE'\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes dice loss with cross entropy loss\n",
    "        -----------\n",
    "        Parameters:\n",
    "            input: Tensor,\n",
    "                predictions tensor\n",
    "            target: Tensor,\n",
    "                ground truth masks tensor\n",
    "        Returns:\n",
    "            The dice loss + cross entropy loss value\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(input):\n",
    "            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(input)))\n",
    "        if not len(input.shape) == 4:\n",
    "            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
    "                              .format(input.shape))\n",
    "        if not input.shape[-2:] == target.shape[-2:]:\n",
    "            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
    "                              .format(input.shape, input.shape))\n",
    "        if not input.device == target.device:\n",
    "            raise ValueError(\n",
    "                \"input and target must be in the same device. Got: {}\" .format(\n",
    "                    input.device, target.device))\n",
    "        # compute softmax over the classes axis\n",
    "        input_soft = torch.nn.functional.softmax(input, dim=1)\n",
    "        input_soft = input_soft.view(input_soft.shape[0], n_classes, -1) # (B, Nclasses, HxW)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = one_hot2D(target, num_classes=input.shape[1],\n",
    "                                  device=input.device, dtype=input.dtype)\n",
    "        target_one_hot = target_one_hot.view(target_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxW)\n",
    "\n",
    "        # compute the actual dice score factors\n",
    "        intersection = torch.sum(input_soft * target_one_hot, 2)\n",
    "        cardinality = torch.sum(input_soft + target_one_hot, 2) # (B)\n",
    "\n",
    "        # count n element for each class\n",
    "        if self.weighted:\n",
    "            counts = torch.sum(target_one_hot, dim=2)\n",
    "            weights = torch.as_tensor(1. / (counts ** 2), dtype=float)\n",
    "            weights = torch.where(torch.isfinite(weights), weights, self.eps) # (B, Nclasses)\n",
    "            # apply weights\n",
    "            intersection = torch.sum(weights*intersection, axis=-1)\n",
    "            cardinality = torch.sum(weights*cardinality, axis=-1)\n",
    "            # compute dice score\n",
    "            dice_score = 1 - 2. * intersection / cardinality\n",
    "            dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "        else:\n",
    "            # compute dice score\n",
    "            dice_score = 1 - 2. * intersection / (cardinality + self.eps)\n",
    "            dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "\n",
    "        return torch.mean(dice_score) + self.CE(input, target).item()\n",
    "    \n",
    "    \n",
    "class CE_loss(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross entropy Loss\n",
    "    -----------\n",
    "    Parameters:\n",
    "        weighted: bool,\n",
    "            Flag to apply or not weights to the labels\n",
    "        weight: Tensor,\n",
    "            tensor with number of classes as shape and containing their weights\n",
    "    \"\"\"\n",
    "    def __init__(self, weighted, weight=None) -> None:\n",
    "        super(CE_loss, self).__init__()\n",
    "        if not weighted:\n",
    "            self.__class__.__name__ = 'CrossEntropyLoss'\n",
    "            self.CE = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.__class__.__name__ = 'WeightedCrossEntropyLoss'\n",
    "            self.CE = nn.CrossEntropyLoss(weight)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes cross entropy loss\n",
    "        -----------\n",
    "        Parameters:\n",
    "            input: Tensor,\n",
    "                predictions tensor\n",
    "            target: Tensor,\n",
    "                ground truth masks tensor\n",
    "        Returns:\n",
    "            The cross entropy loss value\n",
    "        \"\"\"\n",
    "        return self.CE(input, target)\n",
    "    \n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    \"\"\"\n",
    "    Focal Loss\n",
    "    -----------\n",
    "    Parameters:\n",
    "        weight: Tensor,\n",
    "            tensor with number of classes as shape and containing their weights\n",
    "        gamma: float, Optional\n",
    "            gamma coefficient\n",
    "        reduction: str, Optional,\n",
    "            reduction method for Cross entropy\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight # weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Computes focal loss\n",
    "        -----------\n",
    "        Parameters:\n",
    "            input: Tensor,\n",
    "                predictions tensor\n",
    "            target: Tensor,\n",
    "                ground truth masks tensor\n",
    "        Returns:\n",
    "            The focal loss value\n",
    "        \"\"\"\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n",
    "    \n",
    "def compute_loss(outputs, masks, criterion):\n",
    "    \"\"\"\n",
    "    Function to handle different losses computation.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        outputs: Tensor,\n",
    "            prediction tensor\n",
    "        masks: Tensor,\n",
    "            ground truth masks tensor\n",
    "        criterion: loss object,\n",
    "            the loss function class\n",
    "    Returns:\n",
    "        The loss value            \n",
    "    \"\"\"\n",
    "    # Handle BCE\n",
    "    if loss_name == 'BCElogits':\n",
    "        masks_for_loss = masks.squeeze(1).long() \n",
    "        masks_for_loss = one_hot2D(masks_for_loss, n_classes, device, masks_for_loss.dtype) \n",
    "        outputs_for_loss = torch.nn.functional.softmax(outputs, dim=1).float() \n",
    "        return criterion(outputs_for_loss, masks_for_loss)\n",
    "    # Handle other losses\n",
    "    else:\n",
    "        return criterion(outputs.float(), masks.squeeze(1).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INSi_3N6Gj4B"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-u8k0SkunIo"
   },
   "source": [
    "## Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "v64tQz-quolH"
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(pred, mask):\n",
    "    \"\"\"\n",
    "    Computes pixel accuracy.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        pred: Tensor,\n",
    "            predictions tensor\n",
    "        mask: Tensor,\n",
    "            ground truth mask tensor\n",
    "    Returns:\n",
    "        The pixel accuracy value\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(torch.nn.functional.softmax(pred, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def mIoU(pred, mask, smooth=1e-10, n_classes=3):\n",
    "    \"\"\"\n",
    "    Computes mean Intersection over Union.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        pred: Tensor,\n",
    "            predictions tensor\n",
    "        mask: Tensor,\n",
    "            ground truth mask tensor\n",
    "        smooth: float, Optional,\n",
    "            smoothing factor\n",
    "        n_classes: int, Optional,\n",
    "            number of classes\n",
    "    Returns:\n",
    "        The pixel accuracy value\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred_mask = torch.argmax(torch.nn.functional.softmax(pred, dim=1), dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    \"\"\"\n",
    "    Gets the learning rate recommended by the optimizer\n",
    "    -----------\n",
    "    Parameters:\n",
    "        optimizer: optimizer class\n",
    "    Returns:\n",
    "        The learning rate\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def slice_image(img, axis, slice_n):\n",
    "    \"\"\"\n",
    "    Slices a numpy array image\n",
    "    -----------\n",
    "    Parameters:\n",
    "        img: numpy array,\n",
    "            the image to be slices\n",
    "        axis: str,\n",
    "            axis along which slice the image\n",
    "        slice_n: int,\n",
    "            index of the desired slice\n",
    "    Returns:\n",
    "        The 2D image slice\n",
    "    \"\"\"\n",
    "    if axis == 'x':\n",
    "        return img[int(slice_n), :, :]\n",
    "    elif axis == 'y':\n",
    "        return img[:, int(slice_n), :]\n",
    "    elif axis == 'z':\n",
    "        return img[:, :, int(slice_n)]\n",
    "    \n",
    "            \n",
    "def show_prediction_example(model, device, title='', filename='coronacases_009', slice_n=256):\n",
    "    \"\"\"\n",
    "    Shows prediction example\n",
    "    -----------\n",
    "    Parameters:\n",
    "        model: model object,\n",
    "            model used to predict\n",
    "        device: str,\n",
    "            device on which perform the computations\n",
    "        title: str, Optional,\n",
    "            title for the final plot\n",
    "        filename: str, Optional\n",
    "            name of the patient to be used as example\n",
    "        slice_n: int, Optional,\n",
    "            index of the desired slice\n",
    "    \"\"\"    \n",
    "    idx = [i for i, s in enumerate(zenodo_proc['image']['validation']) if filename in s][0]\n",
    "    \n",
    "    test_ct_scan = np.load(zenodo_proc['image']['validation'][idx] + '/z/256.npy')\n",
    "    test_ct_scan = norm_and_eq(test_ct_scan)\n",
    "    \n",
    "    test_gt = np.load(zenodo_proc['mask']['validation'][idx] + '/z/256.npy') \n",
    "\n",
    "    test = np.zeros((1, 1, target_shape[0], target_shape[1]), dtype='float32')\n",
    "    test[0, :, :, :] = test_ct_scan\n",
    "    test = torch.from_numpy(test).to(device)\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(test)\n",
    "    test_pred = torch.argmax(torch.nn.functional.softmax(test_pred, dim=1), dim=1).detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "    # Plot\n",
    "    f, axarr = plt.subplots(1, 4, figsize=(9,9))\n",
    "    f.suptitle(title, y=0.62)\n",
    "    axarr[0].imshow(test_ct_scan, cmap='gray')\n",
    "    axarr[0].set_xlabel('input')\n",
    "    axarr[1].imshow(test_gt, cmap='gray', vmin=0, vmax=2)\n",
    "    axarr[1].set_xlabel('gt')\n",
    "    axarr[2].imshow(test_pred, cmap='gray', vmin=0, vmax=2)\n",
    "    axarr[2].set_xlabel('prediction')\n",
    "    axarr[3].imshow(np.abs(np.subtract(test_gt, test_pred)), cmap='gray', vmin=0, vmax=2)\n",
    "    axarr[3].set_xlabel('difference')\n",
    "    plt.show()\n",
    "\n",
    "def save_history(filepath, history):\n",
    "    \"\"\"\n",
    "    Saves model history\n",
    "    -----------\n",
    "    Parameters:\n",
    "        filepath: str,\n",
    "            path to store the file\n",
    "        history: dict,\n",
    "            model history\n",
    "    \"\"\"\n",
    "    tmp_file = open(filepath +'.pkl', \"wb\")\n",
    "    pickle.dump(history, tmp_file)\n",
    "    tmp_file.close()\n",
    "\n",
    "def load_history(filepath):\n",
    "    \"\"\"\n",
    "    Loads model history\n",
    "    -----------\n",
    "    Parameters:\n",
    "        filepath: str,\n",
    "            path to store the file\n",
    "    Returns:\n",
    "        The history dictionary\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def plot_graph(f, g, f_label, g_label, title):\n",
    "    \"\"\"\n",
    "    Plots a two function graph\n",
    "    -----------\n",
    "    Parameters:\n",
    "        f: list of float,\n",
    "            first list of y-values to be plotted\n",
    "        g: list of float,\n",
    "            second list of y-values to be plotted\n",
    "        f_label: str,\n",
    "            label for the first function in the plot\n",
    "        g_label: str,\n",
    "            label for the second function in the plot \n",
    "        title: str,\n",
    "            title for the plot\n",
    "    \"\"\"\n",
    "    epochs = range(0,len(f))\n",
    "    plt.plot(epochs, f, 'b', label=f_label)\n",
    "    plt.plot(epochs, g, 'orange', label=g_label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plots history graphs of loss, accuracy and mIoU\n",
    "    -----------\n",
    "    Parameters:\n",
    "        history: dict,\n",
    "            history dictionary\n",
    "    \"\"\"\n",
    "    plot_graph(history['train_loss'], history['val_loss'], 'Training loss', 'Validation loss', 'Training and Validation loss')\n",
    "    plot_graph(history['train_acc'], history['val_acc'], 'Training acc', 'Validation acc', 'Training and Validation pixel accuracy')\n",
    "    plot_graph(history['train_miou'], history['val_miou'], 'Training mIoU', 'Validation mIoU', 'Training and Validation mIoU')\n",
    "    \n",
    "# Optimizer\n",
    "def init_optimizer(model):\n",
    "    \"\"\"\n",
    "    Initializes the optimizer.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        model: model class,\n",
    "            the model to be trained\n",
    "    \"\"\"\n",
    "    if optimizer_name == 'ADAM':\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'ADAMW':\n",
    "        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnbse6JRJ_3w"
   },
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0RF2AUzTKDG1"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------Configuration Dicts---------------------------------------------------------\n",
    "# Losses config dict\n",
    "if 'WDL' in loss_name or 'WCE' in loss_name:\n",
    "    weighted = True \n",
    "else:\n",
    "    weighted = False\n",
    "LOSSES = {\n",
    "    'WDL': DiceLoss2D(weighted),\n",
    "    'DL': DiceLoss2D(weighted),\n",
    "    'CE': CE_loss(weighted),\n",
    "    'BCElogits': nn.BCEWithLogitsLoss(),\n",
    "    'DL+CE': DiceLoss2D_CE(weighted),\n",
    "    'WDL+CE': DiceLoss2D_CE(weighted),\n",
    "    'WCE': CE_loss(weighted, weight=torch.as_tensor([0.15, 0.35, 0.50]).to(device)),\n",
    "    'FOCAL': FocalLoss(gamma=5),\n",
    "    'Sobel': Sobel()\n",
    "}\n",
    "\n",
    "# Datasets config dict\n",
    "DATASETS = {\n",
    "    'zenodo': zenodo_proc,\n",
    "    'challenge': challenge_proc,\n",
    "    'zenodo+challenge': [zenodo_proc, challenge_proc]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc-Ht_Rl-6Le"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train if flag is True\n",
    "if do_train:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Some globals\n",
    "    history = {'train_loss': [], 'val_loss': [],\n",
    "               'train_miou': [], 'val_miou': [],\n",
    "               'train_acc': [], 'val_acc': [],\n",
    "               'lrs': [lr]}\n",
    "    min_loss = float('inf')\n",
    "    max_iou = float('-inf')\n",
    "\n",
    "    # Data Loaders\n",
    "    chosen_dataset = DATASETS[dataset_name]\n",
    "    training_DataLoader, test_DataLoader = init_train_test_loader(chosen_dataset, 'training',\n",
    "                                                                  chosen_dataset, 'validation',\n",
    "                                                                  num_workers=4, size_valid=512*3*1)\n",
    "\n",
    "    # Loss\n",
    "    criterion = LOSSES[loss_name]\n",
    "  \n",
    "    if sobel_loss:\n",
    "        sobel_loss = LOSSES['Sobel'].to(device)\n",
    "\n",
    "    # Model\n",
    "    model, model_name = init_model()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = init_optimizer(model)\n",
    "    \n",
    "    # Scheduler\n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr, epochs=n_epochs, steps_per_epoch=len(training_DataLoader))\n",
    "\n",
    "    print(\"Training: {}\\n\".format(model_name))\n",
    "    time.sleep(0.3)\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        iter = 1\n",
    "        with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}/{n_epochs} - Training\")\n",
    "                # Load data\n",
    "                inputs, masks = batch[0].to(device), batch[1].to(device)\n",
    "                # Forward\n",
    "                outputs = model(inputs.float())            \n",
    "                # Compute loss\n",
    "                loss = compute_loss(outputs, masks, criterion)\n",
    "                # Sobel\n",
    "                if sobel_loss:\n",
    "                    sobel_x, sobel_y = sobel_loss(torch.argmax(outputs, dim=1).unsqueeze(1).float(), masks.float())\n",
    "                    # Update loss with Sobel\n",
    "                    loss += sobel_x + sobel_y\n",
    "                # Update loss for stats\n",
    "                running_loss += loss.item()\n",
    "                # Evaluation metrics\n",
    "                iou_score += mIoU(outputs, masks)\n",
    "                accuracy += pixel_accuracy(outputs, masks)\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step() # Update weight          \n",
    "                optimizer.zero_grad() # Empty gradient\n",
    "                tepoch.set_postfix({'Loss':running_loss/iter,'Acc':accuracy/iter,'IoU':iou_score/iter})\n",
    "                time.sleep(0.1)\n",
    "                iter += 1\n",
    "          \n",
    "        # Validation\n",
    "        iter = 1\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        test_iou_score = 0\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}/{n_epochs} - Validation\")\n",
    "                time.sleep(0.3)\n",
    "                inputs, masks = batch[0].to(device), batch[1].to(device)\n",
    "                # Validation loop\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs.float())\n",
    "                    # Evaluation metrics\n",
    "                    test_iou_score += mIoU(outputs, masks)\n",
    "                    test_accuracy += pixel_accuracy(outputs, masks)\n",
    "                    # Loss\n",
    "                    loss = compute_loss(outputs, masks, criterion)\n",
    "                    # Sobel\n",
    "                    if sobel_loss:\n",
    "                        sobel_x, sobel_y = sobel_loss(torch.argmax(outputs, dim=1).unsqueeze(1).float(), masks.float())\n",
    "                        # Update loss with Sobel\n",
    "                        loss += sobel_x + sobel_y\n",
    "                    # Update loss for stats\n",
    "                    test_loss += loss.item()\n",
    "                    tepoch.set_postfix({'Loss':test_loss/iter,'Acc':test_accuracy/iter,'IoU':test_iou_score/iter})\n",
    "                    time.sleep(0.1)\n",
    "                    iter += 1\n",
    "\n",
    "        # Update scheduler\n",
    "        history['lrs'].append(get_lr(optimizer))\n",
    "        if use_scheduler:\n",
    "            scheduler.step() \n",
    "\n",
    "        #calculation mean for each batch\n",
    "        history['train_loss'].append(running_loss/len(training_DataLoader))\n",
    "        history['val_loss'].append(test_loss/len(test_DataLoader))\n",
    "\n",
    "        #iou\n",
    "        history['val_miou'].append(test_iou_score/len(test_DataLoader))\n",
    "        history['train_miou'].append(iou_score/len(training_DataLoader))\n",
    "        history['train_acc'].append(accuracy/len(training_DataLoader))\n",
    "        history['val_acc'].append(test_accuracy/len(test_DataLoader))\n",
    "    \n",
    "        # Save model\n",
    "        '''\n",
    "        # Save by best loss\n",
    "        if min_loss >= test_loss/len(test_DataLoader):\n",
    "            min_loss = test_loss/len(test_DataLoader)\n",
    "            if save_training:\n",
    "                save_checkpoint(model, model_name + '_best')\n",
    "                print('New best Val Loss: {:.6f} at epoch {}'.format(min_loss, epoch+1))\n",
    "        elif save_training:\n",
    "            save_checkpoint(model, model_name + '_checkpoint')\n",
    "        '''  \n",
    "        # Save by best mIoU\n",
    "        if max_iou <= test_iou_score/len(test_DataLoader):\n",
    "            max_iou = test_iou_score/len(test_DataLoader)\n",
    "            if save_training:\n",
    "                save_checkpoint(model, model_name + '_best')\n",
    "                print('New best Val IoU: {:.6f} at epoch {}'.format(max_iou, epoch+1))\n",
    "        elif save_training:\n",
    "            save_checkpoint(model, model_name + '_checkpoint')\n",
    "            \n",
    "        show_prediction_example(model, device, 'Result for epoch: '+str(epoch+1), 'coronacases_009')\n",
    "    \n",
    "        if save_training:\n",
    "            save_history(model_save_path + model_name + '/' + model_name + '_history', history)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kacbjIgfGqho",
    "outputId": "9268bd14-77af-4542-bec5-406674f1dc22"
   },
   "outputs": [],
   "source": [
    "# Do fine tuning is flag is True\n",
    "if do_fine_tuning:\n",
    "    model_config = {\n",
    "        'model_name': '',  # name of the model without prefix and processing to be loaded\n",
    "        'approach': approach,  # 2.5D, transformer\n",
    "        'version': 'checkpoint', # By default uses the saved checkpoint\n",
    "        'fine_tuning': False, # True if the model has been already fine tuned once\n",
    "        'criterion': LOSSES[loss_name].__class__.__name__ , # Loss function\n",
    "        'vit_name': vit_name, # Vit model name, used only if approach:'transformer'\n",
    "        'n_skip': n_skip, # Vit n of skip, used only if approach:'transformer'\n",
    "        'vit_patches_size': vit_patches_size, # vit size of patches, used only if approach:'transformer'\n",
    "        'encoder_name': encoder_name, # name of the encoder, used only if approach:'2.5D'\n",
    "        'encoder_weights': encoder_weights, # weights for the encoder, used only if approach:'2.5D'\n",
    "        'decoder_name': 'Unet', # name of the decoder, used only if approach:'2.5D'\n",
    "        'attention': None, # uses or not the attention layers, used only if approach:'2.5D'\n",
    "        'batch_norm': True, # batch_normalization technique, used only if approach:'2.5D'\n",
    "        'weights_path': model_save_path + '', # path to weights to be fine tuned\n",
    "        'norm_method': norm_method, # normalization method\n",
    "        'lower': lower, # lower bound for normalization, used only if norm_method:'in_range'\n",
    "        'higher': higher, # upper bound for normalization, used only if norm_method:'in_range'\n",
    "        'eq_method': eq_method, # equalization method\n",
    "        'prefix': '' # custom prefix of the model to be loaded\n",
    "    }\n",
    "    \n",
    "    # Stores the previous normalization and equalization methods and changes with those \n",
    "    # defined in the model config avoiding to use the wrong ones\n",
    "    prev_eq_method = eq_method\n",
    "    prev_norm_method = norm_method\n",
    "    prev_lower, prev_higher = lower, higher\n",
    "    \n",
    "    eq_method = model_config['eq_method']\n",
    "    norm_method = model_config['norm_method']\n",
    "    lower, higher = model_config['lower'], model_config['higher'] \n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Some globals\n",
    "    history = {'train_loss': [], 'val_loss': [],\n",
    "               'train_miou': [], 'val_miou': [],\n",
    "               'train_acc': [], 'val_acc': [],\n",
    "               'lrs': [lr]}\n",
    "    min_loss = float('inf')\n",
    "    max_IoU = float('-inf')\n",
    "\n",
    "    # Data Loaders\n",
    "    chosen_dataset = DATASETS[dataset_name]\n",
    "    training_DataLoader, test_DataLoader = init_train_test_loader(chosen_dataset, 'training',\n",
    "                                                                  chosen_dataset, 'validation',\n",
    "                                                                  num_workers=4, size_valid=512*3*2)\n",
    "\n",
    "    # Loss\n",
    "    criterion = LOSSES[loss_name]\n",
    "  \n",
    "    if sobel_loss:\n",
    "        sobel_loss = LOSSES['Sobel'].to(device)\n",
    "\n",
    "    # Model\n",
    "    model, model_name = load_model(model_config)\n",
    "    if freeze_encoder:\n",
    "        print(\"Freezing encoders..\")\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    model_name = finetuning_prefix_name + model_name.split('_'+model_config['version'])[0]\n",
    "    # Optimizer\n",
    "    optimizer = init_optimizer(model)\n",
    "    # Scheduler\n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr, epochs=n_epochs, steps_per_epoch=len(training_DataLoader))\n",
    "\n",
    "    print(\"Training: {}\\n\".format(model_name))\n",
    "    time.sleep(0.3)\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        iter = 1\n",
    "        with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}/{n_epochs} - Training\")\n",
    "                # Load data\n",
    "                inputs, masks = batch[0].to(device), batch[1].to(device)\n",
    "                # Forward\n",
    "                outputs = model(inputs.float())\n",
    "                # Compute loss\n",
    "                loss = compute_loss(outputs, masks, criterion)\n",
    "                # Sobel\n",
    "                if sobel_loss:\n",
    "                    sobel_x, sobel_y = sobel_loss(torch.argmax(outputs, dim=1).unsqueeze(1).float(), masks.float())\n",
    "                    # Update loss with Sobel\n",
    "                    loss += sobel_x + sobel_y\n",
    "                # Update loss for stats\n",
    "                running_loss += loss.item()\n",
    "                # Evaluation metrics\n",
    "                iou_score += mIoU(outputs, masks)\n",
    "                accuracy += pixel_accuracy(outputs, masks)\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step() # Update weight          \n",
    "                optimizer.zero_grad() # Empty gradient\n",
    "                tepoch.set_postfix({'Loss':running_loss/iter,'Acc':accuracy/iter,'IoU':iou_score/iter})\n",
    "                time.sleep(0.1)\n",
    "                iter += 1\n",
    "          \n",
    "        # Validation\n",
    "        iter = 1\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        test_iou_score = 0\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch+1}/{n_epochs} - Validation\")\n",
    "                time.sleep(0.3)\n",
    "                inputs, masks = batch[0].to(device), batch[1].to(device)\n",
    "                # Validation loop\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs.float())\n",
    "                    # Evaluation metrics\n",
    "                    test_iou_score += mIoU(outputs, masks)\n",
    "                    test_accuracy += pixel_accuracy(outputs, masks)\n",
    "                    # Loss\n",
    "                    loss = compute_loss(outputs, masks, criterion)\n",
    "                    # Sobel\n",
    "                    if sobel_loss:\n",
    "                        sobel_x, sobel_y = sobel_loss(torch.argmax(outputs, dim=1).unsqueeze(1).float(), masks.float())\n",
    "                        # Update loss with Sobel\n",
    "                        loss += sobel_x + sobel_y\n",
    "                    # Update loss for stats\n",
    "                    test_loss += loss.item()\n",
    "                    tepoch.set_postfix({'Loss':test_loss/iter,'Acc':test_accuracy/iter,'IoU':test_iou_score/iter})\n",
    "                    time.sleep(0.1)\n",
    "                    iter += 1\n",
    "\n",
    "        # Update scheduler\n",
    "        history['lrs'].append(get_lr(optimizer))\n",
    "        if use_scheduler:\n",
    "            scheduler.step() \n",
    "\n",
    "        #calculation mean for each batch\n",
    "        history['train_loss'].append(running_loss/len(training_DataLoader))\n",
    "        history['val_loss'].append(test_loss/len(test_DataLoader))\n",
    "\n",
    "        #iou\n",
    "        history['val_miou'].append(test_iou_score/len(test_DataLoader))\n",
    "        history['train_miou'].append(iou_score/len(training_DataLoader))\n",
    "        history['train_acc'].append(accuracy/len(training_DataLoader))\n",
    "        history['val_acc'].append(test_accuracy/len(test_DataLoader))\n",
    "    \n",
    "        # Save model\n",
    "        '''\n",
    "        # Save by best loss\n",
    "        if min_loss >= test_loss/len(test_DataLoader):\n",
    "            min_loss = test_loss/len(test_DataLoader)\n",
    "            if save_training:\n",
    "                save_checkpoint(model, model_name + '_best')\n",
    "                print('New best Val Loss: {:.6f} at epoch {}'.format(min_loss, epoch+1))\n",
    "        elif save_training:\n",
    "            save_checkpoint(model, model_name + '_checkpoint')\n",
    "        '''     \n",
    "        # Save by best mIoU\n",
    "        if max_IoU <= test_iou_score/len(test_DataLoader):\n",
    "            max_IoU = test_iou_score/len(test_DataLoader)\n",
    "            if save_training:\n",
    "                save_checkpoint(model, model_name + '_best')\n",
    "                print('New best Val IoU: {:.6f} at epoch {}'.format(max_IoU, epoch+1))\n",
    "                \n",
    "        show_prediction_example(model, device, 'Result for epoch: '+str(epoch+1), 'coronacases_009')\n",
    "    \n",
    "        if save_training:\n",
    "            save_history(model_save_path + model_name + '/' + model_name + '_history', history)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    plot_history(history)\n",
    "\n",
    "\n",
    "    eq_method = prev_eq_method \n",
    "    norm_method = prev_norm_method\n",
    "    lower, higher  = prev_lower, prev_higher "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6W5zl_yGmr1"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVID_PredictionDataLoader():\n",
    "    \"\"\"\n",
    "    Class to handle datasets for predictions. Different from the previous one, this class \n",
    "    is meant to pass, as input, 2D images in the correct order to recreate the original volumes\n",
    "    -----------\n",
    "    Parameters:\n",
    "        dataset: dict or list of dict,\n",
    "            dataset dict with all the relative paths\n",
    "        dts_type: str or list of str,\n",
    "            defining the dataset split folder, 'training' or 'validation'\n",
    "        shape: tuple,\n",
    "            images target shape\n",
    "        size_n_patients: int, Optional,\n",
    "            number of patients to work with, with default value -1 it uses all the patients\n",
    "    \"\"\"  \n",
    "    def __init__(self, dataset, dts_type, shape, size_n_patients=-1):\n",
    "        self.dataset = dataset\n",
    "        self.dts_type = dts_type\n",
    "        self.shape = shape\n",
    "        self.all_x = {}\n",
    "        self.all_y = {}\n",
    "        self.size_n_patients = size_n_patients\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.mean = 0\n",
    "        self.std = 0\n",
    "        self.n_patiens = 0\n",
    "        self.__getdataset__()\n",
    "\n",
    "    def __getdataset__(self):\n",
    "        if isinstance(self.dts_type, str):\n",
    "            self.__populate_dataset__(self.dts_type)\n",
    "        else:\n",
    "            for folder in self.dts_type:\n",
    "                self.__populate_dataset__(folder)\n",
    "            self.mean /= len(self.dts_type)\n",
    "            self.std /= len(self.dts_type)\n",
    "   \n",
    "\n",
    "    def __populate_dataset__(self, folder):\n",
    "        # Handle unique dataset\n",
    "        if not isinstance(self.dataset, list):\n",
    "            counter = 0\n",
    "            for patient_x, patient_y in zip(self.dataset['image'][folder],self.dataset['mask'][folder]):\n",
    "                self.all_x[patient_x] = {'x':[], 'y':[], 'z':[]}\n",
    "                self.all_y[patient_y] = {'x':[], 'y':[], 'z':[]}\n",
    "                for axis in ['x', 'y', 'z']:\n",
    "                    if axis == 'x':\n",
    "                        n_samp = self.shape[0]\n",
    "                    elif axis == 'y':\n",
    "                        n_samp = self.shape[1]\n",
    "                    elif axis == 'z':\n",
    "                        n_samp = self.shape[2]\n",
    "                    for slice_n in range(n_samp):\n",
    "                        self.all_x[patient_x][axis].append(patient_x + '/' + axis + '/' + str(slice_n) + '.npy')\n",
    "                        self.all_y[patient_y][axis].append(patient_y + '/' + axis + '/' + str(slice_n) + '.npy') \n",
    "                counter += 1\n",
    "                if self.size_n_patients == counter:\n",
    "                    break\n",
    "            self.mean += self.dataset['mean']\n",
    "            self.std += self.dataset['std']\n",
    "        else:\n",
    "            for subset in self.dataset:\n",
    "                counter = 0\n",
    "                for patient_x, patient_y in zip(subsett['image'][folder],subset['mask'][folder]):\n",
    "                    self.all_x[patient_x] = {'x':[], 'y':[], 'z':[]}\n",
    "                    self.all_y[patient_y] = {'x':[], 'y':[], 'z':[]}\n",
    "                    for axis in ['x', 'y', 'z']:\n",
    "                        if axis == 'x':\n",
    "                            n_samp = self.shape[0]\n",
    "                        elif axis == 'y':\n",
    "                            n_samp = self.shape[1]\n",
    "                        elif axis == 'z':\n",
    "                            n_samp = self.shape[2]\n",
    "                        for slice_n in range(n_samp):\n",
    "                            self.all_x[patient_x][axis].append(patient_x + '/' + axis + '/' + str(slice_n) + '.npy')\n",
    "                            self.all_y[patient_y][axis].append(patient_y + '/' + axis + '/' + str(slice_n) + '.npy') \n",
    "                    counter += 1\n",
    "                    if self.size_n_patients == counter:\n",
    "                        break\n",
    "                self.mean += self.dataset['mean']\n",
    "                self.std += self.dataset['std']\n",
    "            self.mean /= len(self.dataset)\n",
    "            self.std /= len(self.dataset) \n",
    "            \n",
    "        self.info = len(self.all_x)\n",
    "        self.n_patients = len(self.all_x)\n",
    "\n",
    "        if len(self.x)!=len(self.y): raise SystemError('Problem with Img and Gt, no same size')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.info\n",
    "\n",
    "    def get_n_patients(self):\n",
    "        return self.n_patients\n",
    "    \n",
    "    def get_patient_path(self, patient_index):\n",
    "        # Get one patient only from the list of all patiens\n",
    "        return list(self.all_y.keys())[patient_index].split(\"mask/\")[-1]\n",
    "\n",
    "    def get_one_axis(self, patient_index, axis):\n",
    "        # Get one axis only among the three axes\n",
    "        patient_x = list(self.all_x.keys())[patient_index]\n",
    "        patient_y = list(self.all_y.keys())[patient_index]\n",
    "        self.x = self.all_x[patient_x][axis]\n",
    "        self.y = self.all_y[patient_y][axis]\n",
    "        self.x.sort(key=natural_keys)\n",
    "        self.y.sort(key=natural_keys)\n",
    "        self.info = len(self.x)\n",
    "\n",
    "    def __getitem__(self, index=None):    \n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.info)\n",
    "        ct_scan = np.load(self.x[index])\n",
    "        ct_scan = norm_and_eq(ct_scan, self.mean, self.std)\n",
    "        \n",
    "        mask = np.load(self.y[index])\n",
    "        ct_scan = torch.from_numpy(ct_scan).unsqueeze(0)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "         \n",
    "        return ct_scan.float(), mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y199NxnleAvs"
   },
   "source": [
    "## Aux functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aLc02IKkeC_L"
   },
   "outputs": [],
   "source": [
    "def predict_one_axis(model, test_Dataset, axis, bs=32):\n",
    "    \"\"\"\n",
    "    Predicts all slices along one axis.\n",
    "    ---------\n",
    "    Parameters:\n",
    "        model: model object,\n",
    "            the model used to predict\n",
    "        test_Dataset: dict,\n",
    "            dataset dictinary with all the relative paths\n",
    "        axis: str,\n",
    "            axis to be predicted\n",
    "        bs: int, Optional,\n",
    "            batch size\n",
    "    Returns:\n",
    "        The prediction and the relative dice score\n",
    "    \"\"\"\n",
    "    # Prepare Dataloader\n",
    "    test_DataLoader = DataLoader(test_Dataset, batch_size=bs, num_workers=4)\n",
    "    # data_array should be in shape (batch_size, channel, height, width)\n",
    "    x_size, y_size, z_size = target_shape\n",
    "    model.eval()\n",
    "    time.sleep(0.3)\n",
    "    with torch.no_grad():\n",
    "        dice_score = 0\n",
    "        iter = 1\n",
    "        prediction = np.zeros([x_size,y_size,z_size])\n",
    "        channel_from = 0\n",
    "        channel_to = bs\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch [1/1] - Predicting axis {axis}\")\n",
    "                predict = model(batch[0].float().to(device))\n",
    "                dice_score += weighted_dice_score_2D(predict.float(), batch[1].squeeze(1).long().to(device))\n",
    "                predict = torch.argmax(predict, dim=1)\n",
    "                predict = predict.cpu().numpy()\n",
    "                if axis == \"x\":\n",
    "                    prediction[channel_from:channel_to,:,:] = predict\n",
    "                elif axis == 'y':\n",
    "                    prediction[:,channel_from:channel_to,:] = predict.transpose([1,0,2])\n",
    "                elif axis == 'z':\n",
    "                    prediction[:,:,channel_from:channel_to] = predict.transpose([1,2,0])\n",
    "                else:\n",
    "                    assert False\n",
    "                channel_from = channel_to\n",
    "                channel_to = channel_to + len(batch[0])\n",
    "            tepoch.set_postfix({'Weighted Dice Accuracy':dice_score/iter})\n",
    "            time.sleep(0.1)\n",
    "            iter += 1\n",
    "    return prediction, dice_score/len(test_DataLoader)\n",
    "\n",
    "\n",
    "def predict_one_patient(model, dataset, dts_type, patient_index, dataset_class=None):\n",
    "    \"\"\"\n",
    "    Predicts one patient considering all the slices along all the axes.\n",
    "    ---------\n",
    "    Parameters:\n",
    "        model: model object,\n",
    "            the model used to predict\n",
    "        dataset: dict,\n",
    "            dataset dictinary with all the relative paths\n",
    "        dts_type: str,\n",
    "            dataset split folder, 'training' or 'validation'\n",
    "        patient_index: int,\n",
    "            index of the patient to be predicted\n",
    "        dataset_class: COVID_PredictionDataLoader class,\n",
    "            the COVID_PredictionDataLoader class\n",
    "    Returns:\n",
    "        The three predictions along the three axes, the dice score along each axis and the path \n",
    "        to the predicted patient data\n",
    "    \"\"\"\n",
    "    if dataset_class == None:\n",
    "        dataset_class = COVID_PredictionDataLoader(dataset, dts_type, target_shape)\n",
    "    patient_path = dataset_class.get_patient_path(patient_index)\n",
    "    print(\"Predicting...\")\n",
    "    # Predict x\n",
    "    dataset_class.get_one_axis(patient_index, 'x')\n",
    "    xpred, xdice = predict_one_axis(model, dataset_class, 'x', 32)\n",
    "    # Predict y\n",
    "    dataset_class.get_one_axis(patient_index, 'y')\n",
    "    ypred, ydice = predict_one_axis(model, dataset_class, 'y', 32)\n",
    "    # predict z\n",
    "    dataset_class.get_one_axis(patient_index, 'z')\n",
    "    zpred, zdice = predict_one_axis(model, dataset_class, 'z', 32)  \n",
    "    return xpred, ypred, zpred, [xdice, ydice, zdice], patient_path\n",
    "\n",
    "\n",
    "def predict_all_patients(dataset_mask_root, dataset, dts_type, model_config, size_n_patients=-1):\n",
    "    \"\"\"\n",
    "    Predicts all the patients considering all the slices along all the axes.\n",
    "    ---------\n",
    "    Parameters:\n",
    "        dataset_mask_root: str,\n",
    "            path to the raw ground truth masks\n",
    "        dataset: dict,\n",
    "            dataset dictinary with all the relative paths\n",
    "        dts_type: str,\n",
    "            dataset split folder, 'training' or 'validation'\n",
    "        model_config: dict,\n",
    "            dict with all the information needed about the model\n",
    "        size_n_patients: int, Optional,\n",
    "            number of patients to be predicted\n",
    "    Returns:\n",
    "        A dict with the 2D dice score along each axis, the 3D dice score along each axis, the \n",
    "        total 3D dice score, the total 3D weighted dice score, the 3D dice score of each label, \n",
    "        the Pearson Correlation Coefficient, the precision and recall values and the \n",
    "        confusion matrix\n",
    "    \"\"\"    \n",
    "    test_Dataset = COVID_PredictionDataLoader(dataset, dts_type, target_shape, size_n_patients)\n",
    "    tot_x_dice2D, tot_y_dice2D, tot_z_dice2D = 0, 0, 0\n",
    "    tot_x_dice3D, tot_y_dice3D, tot_z_dice3D = 0, 0 ,0\n",
    "    dice3D, weighted_dice3D, pcc = 0, 0, 0\n",
    "    each_label_dice = np.zeros(n_classes)\n",
    "    pcc = 0\n",
    "    cm = np.zeros((n_classes,n_classes))\n",
    "    # Load model\n",
    "    model, _ = load_model(model_config)\n",
    "    n_patients = test_Dataset.get_n_patients()\n",
    "    for i in range(n_patients):\n",
    "        print(\"Predicting patient \", i+1)\n",
    "        xpred, ypred, zpred, scores, patient_path = predict_one_patient(model, dataset, dts_type, i, test_Dataset)\n",
    "        print(\"Combining predictions\")\n",
    "        if merging_method == 'bagging':\n",
    "            print(\"    Bagging\")\n",
    "            pred = bagging(xpred, ypred, zpred)   \n",
    "        elif merging_method == 'boosting':\n",
    "            print(\"    Boosting\")\n",
    "            pred = boosting(xpred, ypred, zpred, scores)   \n",
    "        elif merging_method == 'threshold bagging':\n",
    "            print(\"    Bagging with Threshold\")\n",
    "            pred = bagging_with_threshold(xpred, ypred, zpred)\n",
    "                      \n",
    "        gt = load_scan(dataset_mask_root + patient_path + '.nii.gz')\n",
    "        \n",
    "        'Option 1: Rescale the GT'\n",
    "        #print(\"Rescaling GT\")  \n",
    "        #gt = rescale_one_gt(gt)\n",
    "        \n",
    "        'Option 2: Rescale the Prediction'\n",
    "        print(\"Rescaling predictions\")                \n",
    "        pred = rescale_to_original(pred, (334/512, 334/512, 1), target_resolution=gt.header.get_zooms(), target_shape=gt.shape)\n",
    "        xpred = rescale_to_original(xpred, (334/512, 334/512, 1), target_resolution=gt.header.get_zooms(), target_shape=gt.shape).round()\n",
    "        ypred = rescale_to_original(ypred, (334/512, 334/512, 1), target_resolution=gt.header.get_zooms(), target_shape=gt.shape).round()\n",
    "        zpred = rescale_to_original(zpred, (334/512, 334/512, 1), target_resolution=gt.header.get_zooms(), target_shape=gt.shape).round()\n",
    "        gt = gt.get_fdata()\n",
    "\n",
    "        gt = gt.round()\n",
    "        pred = pred.round()\n",
    "        \n",
    "        tot_x_dice2D += scores[0]\n",
    "        tot_y_dice2D += scores[1]\n",
    "        tot_z_dice2D += scores[2]\n",
    "            \n",
    "        print(\"Computing each prediction 3D Weighted Dice Score\")\n",
    "        tot_x_dice3D = weighted_dice_score_3D(xpred, gt)\n",
    "        tot_y_dice3D = weighted_dice_score_3D(ypred, gt)\n",
    "        tot_z_dice3D = weighted_dice_score_3D(zpred, gt)\n",
    "        print(\"Computing final prediction 3D Dice Scores\")        \n",
    "        weighted_dice3D += weighted_dice_score_3D(pred, gt)\n",
    "        dice3D += dice_score_3D(pred, gt)\n",
    "        print(\"Computing each label Dice score\")\n",
    "        each_label_dice += each_label_dice_score_3D(pred, gt)\n",
    "        print(\"Computing final prediction Pearson Correlation Coefficient\")        \n",
    "        pcc += np.corrcoef(pred.flatten(), gt.flatten())[0][1]        \n",
    "        print(\"Computing final prediction Confusion Matrix\\n\")        \n",
    "        cm += confusion_matrix(pred.flatten(), gt.flatten(), normalize='true')\n",
    "        \n",
    "    cm /= n_patients\n",
    "    \n",
    "    print(\"Computing Precision & Recall\\n\") \n",
    "    true_pos = np.diag(cm)\n",
    "    false_pos = np.sum(cm, axis=0) - true_pos\n",
    "    false_neg = np.sum(cm, axis=1) - true_pos\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    \n",
    "    each_label_dice /= n_patients\n",
    "    \n",
    "    return {'2D Dice x':tot_x_dice2D/n_patients, '2D Dice y':tot_y_dice2D/n_patients, '2D Dice z':tot_z_dice2D/n_patients,\n",
    "            '3D Dice x':tot_x_dice3D/n_patients, '3D Dice y':tot_y_dice3D/n_patients, '3D Dice z':tot_z_dice3D/n_patients,\n",
    "            '3D Dice':dice3D/n_patients, '3D Weighted Dice':weighted_dice3D/n_patients, 'Each Label Dice': each_label_dice,\n",
    "            'PCC':pcc/n_patients, 'precision':precision, 'recall':recall, 'CM':cm}\n",
    "    \n",
    "\n",
    "def predict_one_axis_softmax(model, test_Dataset, axis, axis_w, class_w, bs=4, prediction=None):\n",
    "    \"\"\"\n",
    "    Predicts all slices along one axis for the probabilities combination method.\n",
    "    ---------\n",
    "    Parameters:\n",
    "        model: model object,\n",
    "            the model used to predict\n",
    "        test_Dataset: dict,\n",
    "            dataset dictinary with all the relative paths\n",
    "        axis: str,\n",
    "            axis to be predicted\n",
    "        axis: list of float,\n",
    "            list of weights for each axis\n",
    "        class_w: list of float,\n",
    "            list of weights for each axis\n",
    "        bs: int, Optional,\n",
    "            batch size\n",
    "        prediction: Tensor,\n",
    "            tensor storing previous predicted probabilities\n",
    "    Returns:\n",
    "        The prediction\n",
    "    \"\"\"\n",
    "    # Prepare Dataloader\n",
    "    test_DataLoader = DataLoader(test_Dataset, batch_size=bs, num_workers=4)\n",
    "    # data_array should be in shape (batch_size, channel, height, width)\n",
    "    x_size, y_size, z_size = target_shape\n",
    "    model.eval()\n",
    "    time.sleep(0.3)\n",
    "    with torch.no_grad():\n",
    "        if prediction == None:\n",
    "            prediction = torch.zeros([n_classes,x_size,y_size,z_size]).to(device)\n",
    "        # Load test data\n",
    "        test_DataLoader = DataLoader(test_Dataset, batch_size=bs, num_workers=2)\n",
    "        channel_from = 0\n",
    "        channel_to = bs\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch [1/1] - Predicting axis {axis}\")\n",
    "                predict = model(batch[0].float().to(device))\n",
    "                predict = torch.softmax(predict, dim=1)\n",
    "                if axis == 'x':\n",
    "                    predict *= axis_w[0]\n",
    "                    prediction[0,channel_from:channel_to,:,:] += predict[:,0,:,:] * class_w[0]\n",
    "                    prediction[1,channel_from:channel_to,:,:] += predict[:,1,:,:] * class_w[1]\n",
    "                    prediction[2,channel_from:channel_to,:,:] += predict[:,2,:,:] * class_w[2]\n",
    "                elif axis == 'y':\n",
    "                    predict *= axis_w[1]\n",
    "                    prediction[0,:,channel_from:channel_to,:] += predict[:,0,:,:].permute([1,0,2]) * class_w[0]\n",
    "                    prediction[1,:,channel_from:channel_to,:] += predict[:,1,:,:].permute([1,0,2]) * class_w[1]\n",
    "                    prediction[2,:,channel_from:channel_to,:] += predict[:,2,:,:].permute([1,0,2]) * class_w[2]\n",
    "                elif axis == 'z':\n",
    "                    predict *= axis_w[2]\n",
    "                    prediction[0,:,:,channel_from:channel_to] += predict[:,0,:,:].permute([1,2,0]) * class_w[0]\n",
    "                    prediction[1,:,:,channel_from:channel_to] += predict[:,1,:,:].permute([1,2,0]) * class_w[1]\n",
    "                    prediction[2,:,:,channel_from:channel_to] += predict[:,2,:,:].permute([1,2,0]) * class_w[2]\n",
    "                channel_from = channel_to\n",
    "                channel_to = channel_to + len(batch[0])\n",
    "    return prediction\n",
    "\n",
    "def predict_one_patient_softmax(model, dataset, dts_type, patient_index, axis_w, class_w, dataset_class, bs=32):\n",
    "    \"\"\"\n",
    "    Predicts one patient considering all the slices along all the axes and merges with the \n",
    "    probabilities combination method.\n",
    "    ---------\n",
    "    Parameters:\n",
    "        model: model object,\n",
    "            the model used to predict\n",
    "        dataset: dict,\n",
    "            dataset dictinary with all the relative paths\n",
    "        dts_type: str,\n",
    "            dataset split folder, 'training' or 'validation'\n",
    "        patient_index: int,\n",
    "            index of the patient to be predicted\n",
    "        axis_w: list of float,\n",
    "            list of weights for each axis\n",
    "        class_w: list of float,\n",
    "            list of weights for each class\n",
    "        dataset_class: COVID_PredictionDataLoader class,\n",
    "            the COVID_PredictionDataLoader class\n",
    "        bs: int,\n",
    "            batch size\n",
    "    Returns:\n",
    "        The three predictions along the three axes and the path to the predicted patient data\n",
    "    \"\"\"\n",
    "    if dataset_class == None:\n",
    "        dataset_class = COVID_PredictionDataLoader(dataset, dts_type, target_shape)\n",
    "    patient_path = dataset_class.get_patient_path(patient_index)\n",
    "    print(\"Predicting...\")\n",
    "    # Predict x\n",
    "    dataset_class.get_one_axis(patient_index, 'x')\n",
    "    pred = predict_one_axis_softmax(model, dataset_class, 'x', axis_w=axis_w, class_w=class_w, bs=bs)\n",
    "    # Predict y\n",
    "    dataset_class.get_one_axis(patient_index, 'y')\n",
    "    pred = predict_one_axis_softmax(model, dataset_class, 'y', axis_w=axis_w, class_w=class_w, bs=bs, prediction=pred)\n",
    "    # predict z\n",
    "    dataset_class.get_one_axis(patient_index, 'z')\n",
    "    pred = predict_one_axis_softmax(model, dataset_class, 'z', axis_w=axis_w, class_w=class_w, bs=bs, prediction=pred)\n",
    "    \n",
    "    pred = torch.argmax(pred, dim=0).cpu().numpy()\n",
    "    return pred, patient_path\n",
    "\n",
    "def predict_all_patients_softmax(dataset_mask_root, dataset, dts_type, model_config, axis_w=[1,1,1], \n",
    "                                 class_w=[1,1,1], size_n_patients=-1):\n",
    "    \"\"\"\n",
    "    Predicts all the patients considering all the slices along all the axes,\n",
    "    using the probabilities combination method.\n",
    "    ---------\n",
    "    Parameters:\n",
    "        dataset_mask_root: str,\n",
    "            path to the raw ground truth masks\n",
    "        dataset: dict,\n",
    "            dataset dictinary with all the relative paths\n",
    "        dts_type: str,\n",
    "            dataset split folder, 'training' or 'validation'\n",
    "        model_config: dict,\n",
    "            dict with all the information needed about the model\n",
    "        axis_w: list of float, Optional,\n",
    "            list with weights for each axis\n",
    "        class_w: list of float, Optional,\n",
    "            list with weights for each class\n",
    "        size_n_patients: int, Optional,\n",
    "            number of patients to be predicted\n",
    "    Returns:\n",
    "        A dict with the total 3D dice score, the total 3D weighted dice score, the 3D dice score of each label, \n",
    "        the Pearson Correlation Coefficient, the precision and recall values and the \n",
    "        confusion matrix\n",
    "    \"\"\"    \n",
    "    test_Dataset = COVID_PredictionDataLoader(dataset, dts_type, target_shape, size_n_patients)\n",
    "    dice3D, weighted_dice3D, pcc = 0, 0, 0\n",
    "    each_label_dice = np.zeros(n_classes)\n",
    "    pcc = 0\n",
    "    cm = np.zeros((n_classes,n_classes))\n",
    "    # Load model\n",
    "    model, _ = load_model(model_config)\n",
    "    n_patients = test_Dataset.get_n_patients()\n",
    "    for i in range(n_patients):\n",
    "        print(\"Predicting patient \", i+1)\n",
    "        pred, patient_path = predict_one_patient_softmax(model, dataset, dts_type, i, axis_w=axis_w, class_w=class_w, \n",
    "                                                         dataset_class=test_Dataset)        \n",
    "                      \n",
    "        gt = load_scan(dataset_mask_root + patient_path + '.nii.gz')\n",
    "        \n",
    "        'Option 1: Rescale the GT'\n",
    "        #print(\"Rescaling GT\")  \n",
    "        #gt = rescale_one_gt(gt)\n",
    "        \n",
    "        'Option 2: Rescale the Prediction'\n",
    "        print(\"Rescaling predictions\")                \n",
    "        pred = rescale_to_original(pred, (334/512, 334/512, 1), target_resolution=gt.header.get_zooms(), target_shape=gt.shape)\n",
    "        gt = gt.get_fdata()\n",
    "\n",
    "        gt = gt.round()\n",
    "        pred = pred.round()\n",
    "        \n",
    "        print(\"Computing final prediction 3D Dice Scores\")        \n",
    "        weighted_dice3D += weighted_dice_score_3D(pred, gt)\n",
    "        dice3D += dice_score_3D(pred, gt)\n",
    "        print(\"Computing each label Dice score\")\n",
    "        each_label_dice += each_label_dice_score_3D(pred, gt)\n",
    "        print(\"Computing final prediction Pearson Correlation Coefficient\")        \n",
    "        pcc += np.corrcoef(pred.flatten(), gt.flatten())[0][1]        \n",
    "        print(\"Computing final prediction Confusion Matrix\\n\")        \n",
    "        cm += confusion_matrix(pred.flatten(), gt.flatten(), normalize='true')\n",
    "        \n",
    "    cm /= n_patients\n",
    "    \n",
    "    print(\"Computing Precision & Recall\\n\") \n",
    "    true_pos = np.diag(cm)\n",
    "    false_pos = np.sum(cm, axis=0) - true_pos\n",
    "    false_neg = np.sum(cm, axis=1) - true_pos\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    \n",
    "    each_label_dice /= n_patients\n",
    "    \n",
    "    return {'3D Dice':dice3D/n_patients, '3D Weighted Dice':weighted_dice3D/n_patients, 'Each Label Dice': each_label_dice,\n",
    "            'PCC':pcc/n_patients, 'precision':precision, 'recall':recall, 'CM':cm}\n",
    "\n",
    "def rescale_to_original(array, resolution, target_resolution=(334/512, 334/512, 1), target_shape=(512, 512, 512)):\n",
    "    # pad and rescale the array to the same resolution and shape for further processing.\n",
    "    # input: array must has shape (x, y, z) and resolution is a list or tuple with three elements\n",
    "\n",
    "    original_shape = np.shape(array)\n",
    "    target_volume = (target_resolution[0]*target_shape[0], target_resolution[1]*target_shape[1], target_resolution[2]*target_shape[2])\n",
    "    shape_of_target_volume = (int(target_volume[0]/resolution[0]), int(target_volume[1]/resolution[1]), int(target_volume[2]/resolution[2]))\n",
    "\n",
    "    x = max(shape_of_target_volume[0], original_shape[0]) + 2\n",
    "    y = max(shape_of_target_volume[1], original_shape[1]) + 2\n",
    "    z = max(shape_of_target_volume[2], original_shape[2]) + 2\n",
    "\n",
    "    x_start = int(x/2)-int(original_shape[0]/2)\n",
    "    x_end = x_start + original_shape[0]\n",
    "    y_start = int(y/2)-int(original_shape[1]/2)\n",
    "    y_end = y_start + original_shape[1]\n",
    "    z_start = int(z / 2) - int(original_shape[2] / 2)\n",
    "    z_end = z_start + original_shape[2]\n",
    "\n",
    "    array_intermediate = np.zeros((x, y, z), 'float32')\n",
    "    array_intermediate[x_start:x_end, y_start:y_end, z_start:z_end] = array\n",
    "\n",
    "    x_start = int(x / 2) - int(shape_of_target_volume[0] / 2)\n",
    "    x_end = x_start + shape_of_target_volume[0]\n",
    "    y_start = int(y / 2) - int(shape_of_target_volume[1] / 2)\n",
    "    y_end = y_start + shape_of_target_volume[1]\n",
    "    z_start = int(z / 2) - int(shape_of_target_volume[2] / 2)\n",
    "    z_end = z_start + shape_of_target_volume[2]\n",
    "\n",
    "    array_intermediate = array_intermediate[x_start:x_end, y_start:y_end, z_start:z_end]  # Now the array is padded\n",
    "\n",
    "    # rescaling:\n",
    "    array_standard_xy = np.zeros((target_shape[0], target_shape[1], shape_of_target_volume[2]), 'float32')\n",
    "    for s in range(shape_of_target_volume[2]):\n",
    "        array_standard_xy[:, :, s] = cv2.resize(array_intermediate[:, :, s], (target_shape[0], target_shape[1]), cv2.INTER_LANCZOS4)\n",
    "    array_standard = np.zeros(target_shape, 'float32')\n",
    "    for s in range(target_shape[0]):\n",
    "        array_standard[s, :, :] = cv2.resize(array_standard_xy[s, :, :], (target_shape[2], target_shape[1]), cv2.INTER_LINEAR)\n",
    "\n",
    "    return array_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(x, y, z, filter=True):\n",
    "    \"\"\"\n",
    "    Merges the three predictions accordingly to the bagging method\n",
    "    -----------\n",
    "    Parameters:\n",
    "        x: numpy array,\n",
    "            array with the prediction along the x axis\n",
    "        y: numpy array,\n",
    "            array with the prediction along the y axis\n",
    "        z: numpy array,\n",
    "            array with the prediction along the z axis\n",
    "        filter: bool, Optional,\n",
    "            apply either or not the maximum_filter\n",
    "    Returns:\n",
    "        The merged final prediction\n",
    "    \"\"\"\n",
    "    # Combining x-y-z predictions\n",
    "    final_prediction = np.array((x + y + z) // 3, 'float32')\n",
    "    if filter:\n",
    "        # Filter result\n",
    "        final_prediction = scipy.ndimage.maximum_filter(final_prediction,3)\n",
    "    return final_prediction\n",
    "\n",
    "def bagging_with_threshold(x, y, z, thresholds=[1,3], filter=True):\n",
    "    \"\"\"\n",
    "    Merges the three predictions accordingly to the bagging method \n",
    "    with a threshold\n",
    "    -----------\n",
    "    Parameters:\n",
    "        x: numpy array,\n",
    "            array with the prediction along the x axis\n",
    "        y: numpy array,\n",
    "            array with the prediction along the y axis\n",
    "        z: numpy array,\n",
    "            array with the prediction along the z axis\n",
    "        thresholds: list of int,\n",
    "            threshold values\n",
    "        filter: bool, Optional,\n",
    "            apply either or not the maximum_filter\n",
    "    Returns:\n",
    "        The merged final prediction\n",
    "    \"\"\"\n",
    "    # Combining x-y-z predictions\n",
    "    pred = np.array((x + y + z), 'float32')\n",
    "    pred[pred  <= thresholds[0]] = 0\n",
    "    pred[(pred > thresholds[0]) & (pred <= thresholds[1])] = 1\n",
    "    pred[pred  > thresholds[1]] = 2\n",
    "    if filter:\n",
    "        # Filter result\n",
    "        pred = scipy.ndimage.maximum_filter(pred,3)\n",
    "    return pred\n",
    "\n",
    "def boosting(x_pred, y_pred, z_pred, w, filter=True):\n",
    "    \"\"\"\n",
    "    Merges the three predictions accordingly to the boosting method\n",
    "    -----------\n",
    "    Parameters:\n",
    "        x_pred: numpy array,\n",
    "            array with the prediction along the x axis\n",
    "        y_pred: numpy array,\n",
    "            array with the prediction along the y axis\n",
    "        z_pred: numpy array,\n",
    "            array with the prediction along the z axis\n",
    "        w: list of int,\n",
    "            list of weights, one for each class\n",
    "        filter: bool, Optional,\n",
    "            apply either or not the maximum_filter\n",
    "    Returns:\n",
    "        The merged final prediction\n",
    "    \"\"\"\n",
    "    vote_0 = torch.zeros(x_pred.shape).to(device)\n",
    "    vote_1 = torch.zeros(x_pred.shape).to(device)\n",
    "    vote_2 = torch.zeros(x_pred.shape).to(device)\n",
    "\n",
    "    vote_0[torch.where(torch.from_numpy(x_pred)==0)] += w[0]\n",
    "    vote_0[torch.where(torch.from_numpy(y_pred)==0)] += w[1]\n",
    "    vote_0[torch.where(torch.from_numpy(z_pred)==0)] += w[2]\n",
    "\n",
    "    vote_1[torch.where(torch.from_numpy(x_pred)==1)] += w[0]\n",
    "    vote_1[torch.where(torch.from_numpy(y_pred)==1)] += w[1]\n",
    "    vote_1[torch.where(torch.from_numpy(z_pred)==1)] += w[2]\n",
    "\n",
    "    vote_2[torch.where(torch.from_numpy(x_pred)==2)] += w[0]\n",
    "    vote_2[torch.where(torch.from_numpy(y_pred)==2)] += w[1]\n",
    "    vote_2[torch.where(torch.from_numpy(z_pred)==2)] += w[2]\n",
    "\n",
    "    t_shape = x_pred.shape\n",
    "    vote = torch.zeros((n_classes, t_shape[0], t_shape[1], t_shape[2])).to(device)\n",
    "    vote[0, :, :, :] = vote_0\n",
    "    vote_0 = None\n",
    "    vote[1, :, :, :] = vote_1\n",
    "    vote_1 = None\n",
    "    vote[2, :, :, :] = vote_2\n",
    "    vote_2 = None\n",
    "    vote = torch.argmax(vote, dim=0)\n",
    "    vote = vote.cpu().numpy()\n",
    "\n",
    "    if filter:\n",
    "        # Filter result\n",
    "        vote = scipy.ndimage.maximum_filter(vote,3)\n",
    "    return vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nii_mask(filepath, mask_array, affine=np.eye(4), header=None):\n",
    "    \"\"\"\n",
    "    Saves nii mask\n",
    "    -----------\n",
    "    Parameters:\n",
    "        filepath: str,\n",
    "            destination path\n",
    "        mask_array: numpy array,\n",
    "            mask numpy array to be saved\n",
    "        affine: numpy array,\n",
    "            affine matrix for the .nii.gz parameter\n",
    "        header: nibabel header,\n",
    "            header to copy .nii.gz parameters\n",
    "    \"\"\"\n",
    "    img = nib.Nifti1Image(mask_array, affine=affine, header=header)\n",
    "    if header == None:\n",
    "        img.header.get_xyzt_units()\n",
    "    img.to_filename(filepath +'.nii.gz')\n",
    "\n",
    "def save_obj_from_nii(filepath, nii_file):\n",
    "    \"\"\"\n",
    "    Saves an .obj file from .nii.gz\n",
    "    -----------\n",
    "    Parameters:\n",
    "        filepath: str,\n",
    "            destination path\n",
    "        nii_file: nibabel object,\n",
    "            .nii.gz object to be converted as .obj\n",
    "    \"\"\"\n",
    "    verts, faces, normals, values = measure.marching_cubes_lewiner(nii_file.get_fdata(), 0)\n",
    "    faces=faces +1\n",
    "    thefile = open(filepath + '.obj', 'w')\n",
    "    for item in verts:\n",
    "        thefile.write(\"v {0} {1} {2}\\n\".format(item[0],item[1],item[2]))\n",
    "    for item in normals:\n",
    "        thefile.write(\"vn {0} {1} {2}\\n\".format(item[0],item[1],item[2]))\n",
    "    for item in faces:\n",
    "        thefile.write(\"f {0}//{0} {1}//{1} {2}//{2}\\n\".format(item[0],item[1],item[2]))  \n",
    "    thefile.close()\n",
    "\n",
    "def show_3D_plot(nii_img):\n",
    "    \"\"\"\n",
    "    Plots a 3D graph of a .nii.gz file.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        nii_img: nibabel object,\n",
    "            .nii.gz file to be plotted\n",
    "    \"\"\"\n",
    "    verts, faces, normals, values = measure.marching_cubes_lewiner(nii_img.get_fdata(), 0)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_trisurf(verts[:, 0], verts[:,1], faces, verts[:, 2],\n",
    "                    linewidth=0.2, antialiased=True)\n",
    "    plt.show()\n",
    "\n",
    "def decode_segmap(image, nc=3, colors=[(0, 0, 0), (0, 128, 0), (128, 0, 0)]):\n",
    "    \"\"\"\n",
    "    Assigns a color map to a mask\n",
    "    -----------\n",
    "    Parameters:\n",
    "        image: numpy array,\n",
    "            mask numpy array\n",
    "        nc: int,\n",
    "            number of classes\n",
    "        colors: list of int tuples,\n",
    "            list of rgb values tuples for each label\n",
    "    Returns:\n",
    "        The colored mask\n",
    "    \"\"\"\n",
    "    label_colors = np.array(colors) # 0=background # 1=lung, 2=infection\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in range(0, nc):\n",
    "        idx = image == l\n",
    "        r[idx] = label_colors[l, 0]\n",
    "        g[idx] = label_colors[l, 1]\n",
    "        b[idx] = label_colors[l, 2]\n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def show_final_results(ct, gt, pred, n):\n",
    "    \"\"\"\n",
    "    Shows an example of obtained result.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        ct: numpy array,\n",
    "            original ct-scan numpy array,\n",
    "        gt: numpy array,\n",
    "            ground truth numpy array\n",
    "        pred: numpy array,\n",
    "            final prediction,\n",
    "        n: int,\n",
    "            number of samples\n",
    "    \"\"\"\n",
    "    shape = ct.shape\n",
    "    for i in range(n):\n",
    "        nice_sample = False\n",
    "        while not nice_sample:\n",
    "            # Choose a random slice\n",
    "            axis = np.random.choice(['x','y','z'])\n",
    "            # Take one slice\n",
    "            if axis == 'x':\n",
    "                slice_id = np.random.randint(0,shape[0])\n",
    "                img_slice = ct[slice_id,:,:]\n",
    "                gt_slice = gt[slice_id,:,:]\n",
    "                mask_slice = pred[slice_id,:,:]\n",
    "                y_offest_title = 0.64\n",
    "            elif axis == 'y':\n",
    "                slice_id = np.random.randint(0,shape[2])\n",
    "                img_slice = ct[:,slice_id,:]\n",
    "                gt_slice = gt[:,slice_id,:]\n",
    "                mask_slice = pred[:,slice_id,:]\n",
    "                y_offest_title = 0.64\n",
    "            elif axis == 'z':\n",
    "                slice_id = np.random.randint(0,shape[2])\n",
    "                img_slice = ct[:,:,slice_id]\n",
    "                gt_slice = gt[:,:,slice_id]\n",
    "                mask_slice = pred[:,:,slice_id]\n",
    "                y_offest_title = 0.58\n",
    "            if len(np.unique(mask_slice)) != 1:\n",
    "                nice_sample = True\n",
    "        # Plot\n",
    "        gt_color = decode_segmap(gt_slice)\n",
    "        pred_color = decode_segmap(mask_slice)\n",
    "        f, axarr = plt.subplots(1, 6, figsize=(19,19))\n",
    "        f.suptitle('Slice n. {} on axis {}'.format(slice_id, axis), y=y_offest_title, fontsize=14)\n",
    "        axarr[0].imshow(img_slice, cmap='gray')\n",
    "        axarr[0].set_xlabel('Original image')\n",
    "        axarr[1].imshow(gt_slice, cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[1].set_xlabel('Ground truth')\n",
    "        axarr[2].imshow(img_slice, cmap='gray')\n",
    "        axarr[2].imshow(gt_color, alpha=0.3)\n",
    "        axarr[2].set_xlabel('GT mask applied')\n",
    "        axarr[3].imshow(mask_slice, cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[3].set_xlabel('Prediction')\n",
    "        axarr[4].imshow(img_slice, cmap='gray')\n",
    "        axarr[4].imshow(pred_color, alpha=0.3)\n",
    "        axarr[4].set_xlabel('Predicted mask applied')\n",
    "        axarr[5].imshow(np.abs(gt_slice-mask_slice), cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[5].set_xlabel('Difference') \n",
    "        plt.show()\n",
    "\n",
    "def compare_combination_method(gt, pred1, pred2, n):\n",
    "    \"\"\"\n",
    "    Shows an example of obtained result.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        gt: numpy array,\n",
    "            ground truth numpy array\n",
    "        pred1: numpy array,\n",
    "            first prediction to be compared,\n",
    "        pred2: numpy array,\n",
    "            second prediction to be compared,\n",
    "        n: int,\n",
    "            number of samples\n",
    "    \"\"\"    \n",
    "    shape = gt.shape\n",
    "    for i in range(n):\n",
    "        nice_sample = False\n",
    "        while not nice_sample:\n",
    "            # Choose a random slice\n",
    "            slice_id = np.random.randint(0,shape[2])\n",
    "            axis = np.random.choice(['x','y','z'])\n",
    "            # Take one slice\n",
    "            if axis == 'x':\n",
    "                gt_slice = gt[slice_id,:,:]\n",
    "                pred1_slice = pred1[slice_id,:,:]\n",
    "                pred2_slice = pred2[slice_id,:,:]\n",
    "                y_offest_title = 0.64\n",
    "            elif axis == 'y':\n",
    "                gt_slice = gt[:,slice_id,:]\n",
    "                pred1_slice = pred1[:,slice_id,:]\n",
    "                pred2_slice = pred2[:,slice_id,:]\n",
    "                y_offest_title = 0.64\n",
    "            elif axis == 'z':\n",
    "                gt_slice = gt[:,:,slice_id]\n",
    "                pred1_slice = pred1[:,:,slice_id]\n",
    "                pred2_slice = pred2[:,:,slice_id]\n",
    "            y_offest_title = 0.58\n",
    "            if len(np.unique(gt_slice)) != 1:\n",
    "                nice_sample = True\n",
    "        # Plot\n",
    "        f, axarr = plt.subplots(1, 6, figsize=(19,19))\n",
    "        f.suptitle('Slice n. {} on axis {}'.format(slice_id, axis), y=y_offest_title, fontsize=14)\n",
    "        axarr[0].imshow(gt_slice, cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[0].set_xlabel('Ground truth')\n",
    "        axarr[1].imshow(pred1_slice, cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[1].set_xlabel('Prediction method 1')\n",
    "        axarr[2].imshow(pred2_slice, cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[2].set_xlabel('Prediction method 2')\n",
    "        axarr[3].imshow(np.abs(gt_slice-pred1_slice), cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[3].set_xlabel('Difference GT-Method1') \n",
    "        axarr[4].imshow(np.abs(gt_slice-pred2_slice), cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[4].set_xlabel('Difference GT-Method2') \n",
    "        axarr[5].imshow(np.abs(pred1_slice-pred2_slice), cmap='gray', vmin=0, vmax=2)\n",
    "        axarr[5].set_xlabel('Difference Method1-Method2')  \n",
    "        plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        cm: numpy array,\n",
    "            confusion matrix\n",
    "        classes: list,\n",
    "            list of labels\n",
    "        title: str, Optional,\n",
    "            title of the plot\n",
    "        cmap: matplotlib.cm\n",
    "            colormap for the plot\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVGuUaA38hQ4"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "juYNeE348gaB"
   },
   "outputs": [],
   "source": [
    "def weighted_dice_score_2D(input, target, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the 2D weighted dice score.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        input: Tensor,\n",
    "            prediction tensor\n",
    "        target: Tensor,\n",
    "            ground truth mask tensor\n",
    "        eps: float, Optional,\n",
    "            epsilon factor\n",
    "    Returns:\n",
    "        The 2D weighted dice score\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # compute softmax over the classes axis\n",
    "        input_soft = torch.nn.functional.softmax(input, dim=1)\n",
    "        input_soft = input_soft.view(input_soft.shape[0], n_classes, -1) # (B, Nclasses, HxW)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = one_hot2D(target, num_classes=input.shape[1],\n",
    "                                  device=input.device, dtype=input.dtype)\n",
    "        target_one_hot = target_one_hot.view(target_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxW)\n",
    "\n",
    "        # count n element for each class\n",
    "        counts = torch.sum(target_one_hot, dim=2)\n",
    "        weights = torch.as_tensor(1. / (counts ** 2), dtype=float)\n",
    "        weights = torch.where(torch.isfinite(weights), weights, eps) # (B, Nclasses)\n",
    "\n",
    "        # compute the actual dice score factors\n",
    "        intersection = torch.sum(input_soft * target_one_hot, 2)\n",
    "        cardinality = torch.sum(input_soft + target_one_hot, 2) # (B)\n",
    "\n",
    "        # apply weights\n",
    "        intersection = torch.sum(weights*intersection, axis=-1)\n",
    "        cardinality = torch.sum(weights*cardinality, axis=-1)\n",
    "        # compute dice score\n",
    "        dice_score = 2. * intersection / cardinality\n",
    "        dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "\n",
    "        return torch.mean(dice_score).item()\n",
    "\n",
    "def one_hot_3D(labels, num_classes, device=None, dtype=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Convert BxHxWxD image tensor to one hot encoding image tensor with shape BxNxHxWxD with\n",
    "    B: batch size, H: height, W: width, D: depth and N: number of classes\n",
    "    -----------\n",
    "    Parameters:\n",
    "        labels: Tensor,\n",
    "            BxHxW Tensor with the ground truth labels\n",
    "        num_classes: int,\n",
    "            total number of classes\n",
    "    Returns:\n",
    "        the one hot encoded tensor\n",
    "    \"\"\"\n",
    "    batch_size, height, width, depth = labels.shape\n",
    "    one_hot = torch.zeros((batch_size, num_classes, height, width, depth), device=device)\n",
    "    return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps\n",
    "\n",
    "def weighted_dice_score_3D(input, target, eps=1e-6): \n",
    "    \"\"\"\n",
    "    Computes the 3D weighted dice score.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        input: Tensor,\n",
    "            prediction tensor\n",
    "        target: Tensor,\n",
    "            ground truth mask tensor\n",
    "        eps: float, Optional,\n",
    "            epsilon factor\n",
    "    Returns:\n",
    "        The 3D weighted dice score\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # compute softmax over the classes axis\n",
    "        input_one_hot = torch.from_numpy(input).unsqueeze(0).long().to(device)\n",
    "        input_one_hot = one_hot_3D(input_one_hot, num_classes=n_classes, device=device, dtype=input.dtype)\n",
    "        input_one_hot = input_one_hot.view(input_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxWxD)\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = torch.from_numpy(target).unsqueeze(0).long().to(device)\n",
    "        target_one_hot = one_hot_3D(target_one_hot, num_classes=n_classes, device=device, dtype=input.dtype)\n",
    "        target_one_hot = target_one_hot.view(target_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxWxD)\n",
    "\n",
    "        # count n element for each class\n",
    "        counts = torch.sum(target_one_hot, dim=2)\n",
    "        weights = torch.as_tensor(1. / (counts ** 2), dtype=float)\n",
    "        weights = torch.where(torch.isfinite(weights), weights, eps) # (B, Nclasses)\n",
    "\n",
    "        # compute the actual dice score factors\n",
    "        intersection = torch.sum(input_one_hot * target_one_hot, 2)\n",
    "        cardinality = torch.sum(input_one_hot + target_one_hot, 2) # (B)\n",
    "\n",
    "        # apply weights\n",
    "        intersection = torch.sum(weights*intersection, axis=-1)\n",
    "        cardinality = torch.sum(weights*cardinality, axis=-1)\n",
    "        # compute dice score\n",
    "        dice_score = 2. * intersection / cardinality\n",
    "        dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "\n",
    "        return torch.mean(dice_score).item()\n",
    "\n",
    "def dice_score_3D(input, target, eps=1e-6): \n",
    "    \"\"\"\n",
    "    Computes the 3D dice score.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        input: Tensor,\n",
    "            prediction tensor\n",
    "        target: Tensor,\n",
    "            ground truth mask tensor\n",
    "        eps: float, Optional,\n",
    "            epsilon factor\n",
    "    Returns:\n",
    "        The 3D dice score\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # compute softmax over the classes axis\n",
    "        input_one_hot = torch.from_numpy(input).unsqueeze(0).long().to(device)\n",
    "        input_one_hot = one_hot_3D(input_one_hot, num_classes=n_classes, device=device, dtype=input.dtype)\n",
    "        input_one_hot = input_one_hot.view(input_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxWxD)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = torch.from_numpy(target).unsqueeze(0).long().to(device)\n",
    "        target_one_hot = one_hot_3D(target_one_hot, num_classes=n_classes, device=device, dtype=input.dtype)\n",
    "        target_one_hot = target_one_hot.view(target_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxWxD)\n",
    "\n",
    "        # compute the actual dice score factors\n",
    "        intersection = torch.sum(input_one_hot * target_one_hot, 2)\n",
    "        cardinality = torch.sum(input_one_hot + target_one_hot, 2) # (B)\n",
    "\n",
    "        # compute dice score\n",
    "        dice_score = 2. * intersection / cardinality\n",
    "        dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "\n",
    "        return torch.mean(dice_score).item()\n",
    "    \n",
    "def each_label_dice_score_3D(input, target, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the 3D dice score of each label.\n",
    "    -----------\n",
    "    Parameters:\n",
    "        input: Tensor,\n",
    "            prediction tensor\n",
    "        target: Tensor,\n",
    "            ground truth mask tensor\n",
    "        eps: float, Optional,\n",
    "            epsilon factor\n",
    "    Returns:\n",
    "        The 3D dice scores of each label\n",
    "    \"\"\"\n",
    "    dice_scores = np.zeros(n_classes)\n",
    "    with torch.no_grad():\n",
    "        # compute softmax over the classes axis\n",
    "        input_one_hot = torch.from_numpy(input).unsqueeze(0).long().to(device)\n",
    "        input_one_hot = one_hot_3D(input_one_hot, num_classes=n_classes, device=device, dtype=input.dtype)\n",
    "        input_one_hot = input_one_hot.view(input_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxWxD)\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        target_one_hot = torch.from_numpy(target).unsqueeze(0).long().to(device)\n",
    "        target_one_hot = one_hot_3D(target_one_hot, num_classes=n_classes, device=device, dtype=input.dtype)\n",
    "        target_one_hot = target_one_hot.view(target_one_hot.shape[0], n_classes, -1) # (B, Nclasses, HxWxD)\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            input_one_label = input_one_hot[:,i,:]\n",
    "            target_one_label = target_one_hot[:,i,:]\n",
    "\n",
    "            # compute the actual dice score factors\n",
    "            intersection = torch.sum(input_one_label * target_one_label, 1)\n",
    "            cardinality = torch.sum(input_one_label + target_one_label, 1) # (B)\n",
    "\n",
    "            # compute dice score\n",
    "            dice_score = 2. * intersection / cardinality\n",
    "            dice_score = torch.where(torch.isfinite(dice_score), dice_score, torch.zeros_like(dice_score))\n",
    "            dice_scores[i] = torch.mean(dice_score).item()\n",
    "\n",
    "        return dice_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_T4lWZDfGaS"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01O2nB2kFM-W"
   },
   "source": [
    "### Predict and print stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kelz3krIQ4JK",
    "outputId": "06a461f7-bb6b-4359-c224-44bed5e6f4e9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do prediction if flag is True\n",
    "  \n",
    "model_config = {\n",
    "    'model_name': '', # name of the model without prefix and processing to be loaded\n",
    "    'approach': '2.5D', # 2.5D, transformer\n",
    "    'version': 'best',  # best, checkpoint\n",
    "    'fine_tuning': False, # True if the model has been fine tuned\n",
    "    'freeze_encoder': True, # True if the model has been fine tuned with the encoder weights freezed, used only if fine_tuning:True\n",
    "    'vit_name': 'R50+ViT-B_16', # Vit model name, used only if approach:'transformer'\n",
    "    'n_skip': 2, # Vit n of skip, used only if approach:'transformer'\n",
    "    'vit_patches_size': 16, # vit size of patches, used only if approach:'transformer'\n",
    "    'encoder_name': 'resnet101', # name of the encoder, used only if approach:'2.5D'\n",
    "    'encoder_weights': 'imagenet',  # weights for the encoder, used only if approach:'2.5D'\n",
    "    'decoder_name': 'Unet', # name of the decoder, used only if approach:'2.5D'\n",
    "    'attention': None, # to use or not the attention layers, used only if approach:'2.5D'\n",
    "    'batch_norm': True, # batch_normalization technique, used only if approach:'2.5D'\n",
    "    'weights_path': model_save_path + '', # path to weights to be loaded\n",
    "    'norm_method': 'as_colab', # normalization method\n",
    "    'lower': 0, # lower bound for normalization, used only if norm_method:'in_range'\n",
    "    'higher': 255, # upper bound for normalization, used only if norm_method:'in_range'\n",
    "    'eq_method': 'clahe+histeq', # equalization method\n",
    "    'prefix': '' # custom prefix of the model to be loaded\n",
    "}\n",
    "\n",
    "\n",
    "prediction_dataset = 'challenge' # dataset on which perform the evaluation\n",
    "\n",
    "if do_predict:\n",
    "    \n",
    "    # Stores the previous normalization and equalization methods and changes with those \n",
    "    # defined in the model config avoiding to use the wrong ones\n",
    "    prev_eq_method = eq_method\n",
    "    prev_norm_method = norm_method\n",
    "    prev_lower, prev_higher = lower, higher\n",
    "    \n",
    "    eq_method = model_config['eq_method']\n",
    "    norm_method = model_config['norm_method']\n",
    "    lower, higher = model_config['lower'], model_config['higher'] \n",
    "    \n",
    "    # Predict\n",
    "    if merging_method == 'softmax':\n",
    "        axis_w = [1, 1, 1]\n",
    "        class_w = [1, 1, 1]\n",
    "        if prediction_dataset == 'zenodo':\n",
    "            scores = predict_all_patients_softmax(zenodo_mask_root, zenodo_proc, ['training','validation'], model_config, axis_w, class_w)\n",
    "        elif prediction_dataset == 'challenge':\n",
    "            scores = predict_all_patients_softmax(challenge_mask_root, challenge_proc, 'validation', model_config, axis_w, class_w, size_n_patients=10)\n",
    "    else:\n",
    "        if prediction_dataset == 'zenodo':\n",
    "            scores = predict_all_patients(zenodo_mask_root, zenodo_proc, ['training','validation'], model_config)\n",
    "        elif prediction_dataset == 'challenge':\n",
    "            scores = predict_all_patients(challenge_mask_root, challenge_proc, 'validation', model_config, size_n_patients=10)\n",
    "    \n",
    "    print(\"\\n\\n\\nResults:\\n\")\n",
    "    for key in scores.keys():\n",
    "        if key != 'CM':\n",
    "            print(key, scores[key])\n",
    "        \n",
    "    plot_confusion_matrix(scores['CM'], [0,1,2],\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)\n",
    "        \n",
    "    eq_method = prev_eq_method \n",
    "    norm_method = prev_norm_method\n",
    "    lower, higher  = prev_lower, prev_higher "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rlZ-raIRGZzj",
    "5e3CRiMmHs4T",
    "uQkFrtIAdP7y",
    "88WWUlgwH5FZ",
    "o6OG3Q_HGcp3",
    "1xMLtepsK4Vp",
    "FD-iwCKuVArA",
    "igH9716cKtHP",
    "dL2hgHiGOIIy",
    "VKhgcipHGfHD",
    "wi2jcgQC1aYz",
    "0WP0kZSp5wVP",
    "xcmusTj9XeIT",
    "1soT4NouDLRg",
    "3AVm5uJQ60Dd",
    "INSi_3N6Gj4B",
    "y-u8k0SkunIo",
    "Hnbse6JRJ_3w",
    "l6W5zl_yGmr1",
    "eLhfnMivPi2M",
    "Y199NxnleAvs",
    "AVGuUaA38hQ4",
    "01O2nB2kFM-W",
    "GG2wgzgxFQUr",
    "3HFAiz6lFS6S",
    "64JXKZfL31L9"
   ],
   "name": "EAI_Napoli_v2_9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
